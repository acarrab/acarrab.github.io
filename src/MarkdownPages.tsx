/* Auto generated tsx */

import React from "react";
import ReactMarkdown from "react-markdown";
import { Route, Switch } from "react-router-dom";

export class Page {
    text: string
    name: string
    route: string 
}
export class Directory {
    title: string
    pages: Array<Page>
    children: Array<Directory>
}

export var Content: Directory = {
    title: 'Content',
    pages: [
        {
            name: 'Home',
            text: '<!DOCTYPE html>\n<html>\n<head>\n  <meta charset="utf-8">\n  <meta name="generator" content="pandoc">\n  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">\n  <title></title>\n  <style type="text/css">code{white-space: pre;}</style>\n  <!--[if lt IE 9]>\n    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>\n  <![endif]-->\n</head>\n<body>\n<h1>\nAngelo Carrabba<br /><small style="color: #FF6500; font-weight: 200">Clemson Computer Science Senior</small>\n</h1>\n<h2 id="here-are-some-things-you-can-do-here">Here are some things you can do here</h2>\n<ol type="1">\n<li>Please check out my <a href="#/about">about page</a>, it has some information and project history.</li>\n<li>If you are bored, you can visit my <a href="../Resume/Resume.pdf">Resume</a>.</li>\n<li>If you are really bored, you can look at my <a href="https://github.com/acarrab">GitHub</a>.</li>\n<li>If you are extremely bored, you can browse <a href="https://imgur.com/r/FunnyAnimals">this</a>.</li>\n</ol>\n<h2 id="current-work">Current Work</h2>\n<h3 id="research">Research</h3>\n<ul>\n<li><a href="#/projects/topic_modeling_and_hypothesis_generation">Topic Modeling and Hypothesis Generation (Data Mining Research)</a></li>\n</ul>\n<h3 id="personalacademic">Personal/Academic</h3>\n<ul>\n<li>Learning more about different statistical machine learning processes, particularly deep-learning</li>\n<li><a href="#/projects/deep_learning">more info</a></li>\n</ul>\n<h3 id="past-work">Past Work</h3>\n<ul>\n<li><a href="#/projects/multi-robot_environment">Multi-agent Environment Research (REU)</a></li>\n<li><a href="#/projects/pubsub_architecture_analysis">PubSub Architecture Analysis</a></li>\n</ul>\n</body>\n</html>\n',
            route: '/'
        },
        {
            name: 'About',
            text: '<!DOCTYPE html>\n<html>\n<head>\n  <meta charset="utf-8">\n  <meta name="generator" content="pandoc">\n  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">\n  <title></title>\n  <style type="text/css">code{white-space: pre;}</style>\n  <!--[if lt IE 9]>\n    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>\n  <![endif]-->\n</head>\n<body>\n<h1 id="about-me">About me</h1>\n<p>Hello people, this page is coming to you live from a server somewhere on the interweb!</p>\n<p>Anyways, my name is Angelo and I am a Senior Computer Science Student at Clemson. I do things and some of those things are, well, on this website.</p>\n<h2 id="interests">Interests</h2>\n<p>I started out at Clemson University on a pre-med and computer science track; However, saw the impact that was possible by going with a more computer science oriented approach. Since then, I have worked on and helped with a handful of medically based research projects. I plan on going to graduate school and applying/developing my skills in order to contribute to the progression of the medical field.</p>\n<p>I enjoy working with algorithm design, machine learning, and big data.</p>\n<p><a href="/OldWebsite">Websites</a> and other computer graphics related applications are fun as well.</p>\n<h2 id="projects">Projects</h2>\n<h3 id="honors-thesis-work">Honors Thesis Work</h3>\n<p><em>2017-2018 (present)</em></p>\n<p><strong><a href="#/projects/topic_modeling_and_hypothesis_generation">MORE INFO</a></strong></p>\n<p>Year long project with goal of improving existing techniques for hypothesis generation by extracting relevant information from research papers.</p>\n<p>I am working with big data (1.7 million Medical Research Articles from <a href="https://www.ncbi.nlm.nih.gov/pmc/">PubMed Central</a>)</p>\n<h3 id="software-engineering-intern-at-blackbaud">Software engineering intern at <a href="https://www.blackbaud.com/">Blackbaud</a></h3>\n<p><em>Summer 2017</em></p>\n<figure>\n<img src="../Resources/blackbaud.jpg" />\n</figure>\n<center>\n<em>That is me kneeling at the bottom right</em>\n</center>\n<p>Blackbaud is a company that creates software tailored for nonprofit organizations. The most used product is Raiser\'s Edge, which is used by nonprofits when determining how to most efficiently handle their donors.</p>\n<p>Worked on a Scrum team within an ASP.net web application called Raisers Edge NXT—which makes up about 30% of their sales—with Angular 2 on the front-end and C# and SQL on the back-end. Updated and added health pages that gave developers the tools to better analyze database health and edit database package status. Reduced the number of health pages by making a more centralized location for health data.</p>\n<p>Specifically, wrote JavaScript with Angular 2 Components and Services. As well as JavaScript Jasmine unit tests. On the back-end wrote in C# with SpecFlow unit tests and integration tests. Wrote basic SQL as well.</p>\n<h3 id="research-with-human-centered-cloud-robotics-clemson-university">Research with Human-Centered Cloud Robotics @ Clemson University</h3>\n<p><em>Aug. 2016 - Jan. 2017</em></p>\n<p><strong><a href="#/projects/pubsub_architecture_analysis">MORE INFO</a></strong></p>\n<ul>\n<li>Analyzed pubsub architectures performance in real-time environments, or simulations of real-time environments with no prediction.</li>\n<li>Simulations done in Mininet with python scripts for generating the network topology.</li>\n<li>Looked specifically at MQTT and Kafka</li>\n</ul>\n<h3 id="research-at-rutgers-university-with-dimacs">Research at Rutgers University with <a href="http://dimacs.rutgers.edu/">DIMACS</a></h3>\n<p><em>Summer 2016</em></p>\n<p><strong><a href="#/projects/multi-robot_environment">MORE INFO</a></strong></p>\n<p>Joined research experience for Undergrads program at Rutgers University.</p>\n<ol type="1">\n<li>Wrote a <a href="../Resources/finalResearchPaper.pdf">research paper</a> on results.</li>\n<li>Created a simulation in python, with graphics created with pygame.</li>\n<li>Analyze traffic in a multi-robot environment with multi-agent simulations.</li>\n<li>Specifically looking for sharp-transition in behavior in discretized traffic network when compared to continuous one.</li>\n</ol>\n<h4 id="takeaway">Takeaway</h4>\n<p>Determined through self-created simulations that there were no sharp transition in traffic, based on changes in robot size and start-end locality.</p>\n<h3 id="applied-algorithms-lab-clemson-university">Applied Algorithms Lab @ Clemson University</h3>\n<p><em>Jan 2016 - June 2016</em></p>\n<p>Implemented DTW Search with modifications to improve result quality with noisy signal. For EEG database to add search functionality for Neurologists at MUSC research hospital.</p>\n<h3 id="virtual-environment-group-clemson-university">Virtual Environment Group @ Clemson University</h3>\n<p><em>Sep. 2014 - Aug. 2015</em></p>\n<p>Research Assistant C Designed computer vision software in C, OpenGL, and GLUT. Worked on software for calibrating camera image distortion through analysis of an image of a checkered board. Created edge detection processing.</p>\n</body>\n</html>\n',
            route: '/about'
        }
    ],
    children: [
        {
            title: 'Projects',
            pages: [
                {
                    name: 'Topic Modeling And Hypothesis Generation',
                    text: '<!DOCTYPE html>\n<html>\n<head>\n  <meta charset="utf-8">\n  <meta name="generator" content="pandoc">\n  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">\n  <title></title>\n  <style type="text/css">code{white-space: pre;}</style>\n  <style type="text/css">\ndiv.sourceCode { overflow-x: auto; }\ntable.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {\n  margin: 0; padding: 0; vertical-align: baseline; border: none; }\ntable.sourceCode { width: 100%; line-height: 100%; }\ntd.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }\ntd.sourceCode { padding-left: 5px; }\ncode > span.kw { color: #007020; font-weight: bold; } /* Keyword */\ncode > span.dt { color: #902000; } /* DataType */\ncode > span.dv { color: #40a070; } /* DecVal */\ncode > span.bn { color: #40a070; } /* BaseN */\ncode > span.fl { color: #40a070; } /* Float */\ncode > span.ch { color: #4070a0; } /* Char */\ncode > span.st { color: #4070a0; } /* String */\ncode > span.co { color: #60a0b0; font-style: italic; } /* Comment */\ncode > span.ot { color: #007020; } /* Other */\ncode > span.al { color: #ff0000; font-weight: bold; } /* Alert */\ncode > span.fu { color: #06287e; } /* Function */\ncode > span.er { color: #ff0000; font-weight: bold; } /* Error */\ncode > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */\ncode > span.cn { color: #880000; } /* Constant */\ncode > span.sc { color: #4070a0; } /* SpecialChar */\ncode > span.vs { color: #4070a0; } /* VerbatimString */\ncode > span.ss { color: #bb6688; } /* SpecialString */\ncode > span.im { } /* Import */\ncode > span.va { color: #19177c; } /* Variable */\ncode > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */\ncode > span.op { color: #666666; } /* Operator */\ncode > span.bu { } /* BuiltIn */\ncode > span.ex { } /* Extension */\ncode > span.pp { color: #bc7a00; } /* Preprocessor */\ncode > span.at { color: #7d9029; } /* Attribute */\ncode > span.do { color: #ba2121; font-style: italic; } /* Documentation */\ncode > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */\ncode > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */\ncode > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */\n  </style>\n  <!--[if lt IE 9]>\n    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>\n  <![endif]-->\n</head>\n<body>\n<h1 id="honors-research-project-for-senior-year">Honors Research Project for Senior Year</h1>\n<p>This project is a school year long project in which I am working along side grad students in the ACS Lab (Algorithms and Computational Sciences Lab). I am adding to the work being done by <a href="http://sybrandt.com/">Justin Sybrandt</a> by determining if we can increase the accuracy of Moliere, a hypothesis generation method.</p>\n<p>We are seeing if it helps to extract additional information from the full-texts and, if so, how to best do so.</p>\n<h2 id="progress">Progress</h2>\n<p><em>Last updated: Tuesday, October 30th 2017</em></p>\n<p>All code is parallelized and ran on the <a href="https://www.palmetto.clemson.edu/palmetto/userguide_palmetto_overview.html">Clemson Palmetto Cluster</a> in order to complete tasks in reasonable amount of time. Using <a href="http://mpi4py.readthedocs.io/en/stable/">mpi4py</a> in order to run text extraction on the million documents in parallel on the Clemson Palmetto Cluster</p>\n<p>Started by downloading 1.7 million documents over ftp from <a href="https://www.ncbi.nlm.nih.gov/pmc/">PubMed Central</a></p>\n<ol type="1">\n<li>Wrote code to parse xml documents tree structure with hierarchical selection by walking through xml tree and grabbing what I need (example under <strong>hierarchical selection example</strong> section)\n<ul>\n<li>Down to 1.4 million documents because: some did not have abstracts, were not research papers, or were not in proper xml format.</li>\n</ul></li>\n<li>Wrote and ran code to clean text of unicode expressions and in text equations using regular expressions.</li>\n<li>Used <a href="http://www.nltk.org/">NLTK</a> in order to lemmatize text in order to be passed into the next part of the pipeline, the creation of the n_grams.</li>\n<li>Ran Abstracts through n-gram stage of Moliere Pipeline</li>\n</ol>\n<h2 id="weekly-log">Weekly Log</h2>\n<h3 id="n-gram-creation-using-topmine">N-gram creation using <a href="https://arxiv.org/pdf/1406.6312.pdf">ToPMine</a></h3>\n<p><em>Monday, October 30th</em></p>\n<p>Learned how create n-grams through ToPMine with help of Justin, then ran the process on Abstracts document subset.</p>\n<p>Currently only on Abstract documents and analyzed the validity of the results through randomly looking at generated topics by looking through topics and seeing if topics made sense. All of them, to me, seem to be valid pairings of things that occur in the same statement so I will be moving ahead with running n-gram creation on a subset of the full-texts now.</p>\n<p>Here are some of the topics that were found.</p>\n<pre><code>significantly reduced\ndata were collected\nwa examined\namino acid\nincreased risk\nwa present\ncell type\ncell grown\nclosed loop\ncomplex structure\nset of gene\ntechnique based\nhigh dose\nsurvival rate\nhealth outcome\nhuman cell</code></pre>\n<h4 id="problem-with-loss-of-lines">Problem with loss of lines</h4>\n<p>Some of the abstracts were parsed out entirely, so lines were removed, which made us lose tracking information of the document id. There were about 50 that contained all unique words and we can no longer map back the lines since they are different. In order to resolve this issue, it is required to go back and append a non-unique sentence that will not affect the lines.</p>\n<p>It was recommended to fix this problem by appending <code>the quick brown fox.</code> to each document which are placed on separate lines, which was easy to do with sed via a simple bash script</p>\n<p><strong>quickBrownFoxify.sh</strong></p>\n<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="co">#!/bin/bash</span>\n\n<span class="va">newFile=</span>qbf_<span class="va">$1</span>\n<span class="fu">cp</span> <span class="va">$1</span> <span class="va">$newFile</span>\n<span class="fu">sed</span> -i -e <span class="st">&#39;s/^/Quick Brown Fox. /&#39;</span> <span class="va">$newFile</span></code></pre></div>\n<p>which is called by</p>\n<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="ex">./quickBrownFoxify.sh</span> allAbstracts.txt</code></pre></div>\n<p>Now It must be ran again to create the n-grams.</p>\n<h4 id="creating-random-subset-of-documents-from-full-texts-for-testing">Creating random subset of documents from full-texts for testing</h4>\n<p>Each document is given a random and equal chance of selection while walking through file for a total of 100,000 randomly selected documents.</p>\n<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> random\nn <span class="op">=</span> <span class="dv">1328035</span>\nk <span class="op">=</span> <span class="dv">100000</span>\n\n<span class="cf">with</span> <span class="bu">open</span>(<span class="st">&quot;fullTexts_subset.txt&quot;</span>, <span class="st">&quot;w&quot;</span>) <span class="im">as</span> fout:\n    <span class="cf">with</span> <span class="bu">open</span>(<span class="st">&quot;allFulltexts.txt&quot;</span>, <span class="st">&quot;r&quot;</span>) <span class="im">as</span> fin:\n        <span class="cf">for</span> document <span class="kw">in</span> fin:\n            <span class="co"># gives us proper chance of selecting document</span>\n            <span class="cf">if</span> <span class="dv">0</span> <span class="op">==</span> random.randrange(<span class="dv">0</span>, <span class="bu">int</span>(n <span class="op">/</span> k)) <span class="kw">or</span> n <span class="op">==</span> k:\n                fout.write(document)\n                k <span class="op">-=</span> <span class="dv">1</span>\n                <span class="cf">if</span> k <span class="op">%</span> <span class="dv">10000</span> <span class="op">==</span> <span class="dv">0</span>: <span class="bu">print</span>(<span class="st">&quot;written: &quot;</span> <span class="op">+</span> <span class="bu">str</span>(<span class="dv">100000</span> <span class="op">-</span> k))\n                <span class="cf">if</span> k <span class="op">==</span> <span class="dv">0</span>: <span class="cf">break</span>\n            n <span class="op">-=</span> <span class="dv">1</span></code></pre></div>\n<h3 id="removal-of-files-that-are-too-new-from-the-dataset">Removal of files that are too new from the dataset</h3>\n<p><em>Tuesday, October 24th</em></p>\n<p>Went back through pipeline that has been created so far and added a small amount of code to determine if the publish date of the article was this year as we will be removing those from the data set in order to see if prediction is valid for this year. No need for predicting things we really don\'t know yet, because there is no basis for comparison.</p>\n<p>At the end of this process, it has been determined that there are now only 1.3 million articles left in totality. There are a suspicious amount of articles that are lost to being too new and I will be looking at that later, but for now am just working on lemmatizing the text using different methods.</p>\n<p>Here is the breakdown of documents (that are currently stored in separate files according to the number of processes):</p>\n<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash">[<span class="ex">acarrab@login001</span> lemmatizedText]$ wc -l Abstracts_* <span class="kw">|</span> <span class="fu">grep</span> total\n   <span class="ex">1328035</span> total</code></pre></div>\n<p>Here is the breakdown of how the documents were lost.</p>\n<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash">[<span class="ex">acarrab@login001</span> acarrab]$ wc -l *.log\n  <span class="ex">151427</span> failedDuringParsingOrNoAbstract.log\n   <span class="ex">53872</span> hasAbstractAndFailed.log\n  <span class="ex">164944</span> tooNewButHadAbstract.log\n  <span class="ex">370243</span> total</code></pre></div>\n<p>NOTE: <code>tooNewButHadAbstract.log</code> is not necessarily accurate since it would put 3000 as year if the parsing could not find the date.</p>\n<p>I have looked at lemmatization through the means of using specialist NLP tool in order to use those to lemmatize the text, but to no avail. (A lot more learning than initially thought was needed is needed in order to use their tools).</p>\n<p>In order to get some results, used the NLTK\'s lemmatizer in order lemmatize the text from the research papers. The papers are now ready for the next phase, the building of the n-grams.</p>\n<p>After speaking with Justin, he has seen that the benefits of using the Specialist NLP Tools is actually too slow for our data-set size in the end anyways and NLTK lemmatization does a good job and gives promising and significant results within the research that he has done.</p>\n<h3 id="increasing-valid-parsing-rate">Increasing valid parsing rate</h3>\n<p><em>Monday, October 16th</em></p>\n<p>After running the jobs in order to parse the documents, added basic statistics capturing that are added to files while running batch job on the almost 1.8 million documents. At the end of the run, there are counts for different types of errors</p>\n<ul>\n<li><p>Failure to parse xml errors</p></li>\n<li><p>Had abstract keyword within file, but still failed</p></li>\n<li><p>Failed for other reasons</p></li>\n</ul>\n<p>Given this information, I can now look at the files that failed yet had the abstract keyword and determine how to reduce the number of files that are failed to be parsed.</p>\n<h3 id="quality-analysis-of-xml-parsing">Quality analysis of xml parsing</h3>\n<p><em>Wednesday, October 11th</em></p>\n<p>Went through a random subset of the documents and looked at results from parsing in order to determine whether the parsing was doing what it should be doing. After making some modifications to the code written in the previous week, there is a good likelihood of valid parsing.</p>\n<p>Also inserted failure cases for parse if things like the abstracts were missing.</p>\n<p>Ran parsing parallelized on palmetto cluster and looked determined results so far. More validation should be done in order to make sure that most of the data are being used.</p>\n<p>Files are saved in only abstract format and also full-text and abstract format in order to create distinct sets of documents that can give good comparison.</p>\n<h3 id="heirarchical-parsing-of-xml-documents">Heirarchical parsing of xml documents</h3>\n<p><em>Wednesday, October 4th</em></p>\n<p>Wrote code to parse out unicode characters as well as select out abstracts and other relevant text sections from the millions of documents.</p>\n<p>Created method for parsing out xml documents tags that are needed in with hierarchical selection.</p>\n<h4 id="example-and-explanation">Example and Explanation</h4>\n<p>Wrote code that extracts the following information by populating a data object with the keywords under the <code>&quot;GET&quot;</code> keyword under selection through chains of nested objects. For example, if an article title is found in the xml tree under <code>&lt;article-meta&gt;&lt;title-group&gt;&lt;article-title&gt;</code> tags, then <code>data[&quot;Title&quot;]</code> will be set to an array containing the found data.</p>\n<p>This is a kind of cool way to extract the required information from the xml tree structure. Kind of inspired by <a href="http://graphql.org/">graphql</a> selection statements in order to populate arrays of data.</p>\n<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">chains <span class="op">=</span> [\n    {\n        <span class="st">&quot;article-meta&quot;</span>: {\n            <span class="st">&quot;title-group&quot;</span>: {\n                <span class="st">&quot;article-title&quot;</span>: {\n                    <span class="st">&quot;GET&quot;</span>: <span class="st">&quot;Title&quot;</span>\n                }\n            }\n        },\n        <span class="st">&quot;kwd-group&quot;</span>: {\n            <span class="st">&quot;kwd&quot;</span>: {\n                <span class="st">&quot;GET&quot;</span>: <span class="st">&quot;Keywords&quot;</span>\n            }\n        },\n        <span class="st">&quot;contrib-group&quot;</span>: {\n            <span class="st">&quot;contrib&quot;</span>: {\n                <span class="st">&quot;GETALL&quot;</span>: <span class="st">&quot;Contributors&quot;</span>\n            }\n        },</code></pre></div>\n<h3 id="downloading-documents">Downloading Documents</h3>\n<p><em>Thursday, September 28th</em></p>\n<p>Downloaded the documents over ftp from <a href="https://www.ncbi.nlm.nih.gov/pmc/">PubMed Central</a> and moved them onto the Palmetto Cluster.</p>\n<p>Became familiar with some of <a href="http://www.nltk.org/">NLTK</a> and processes like lemmatization and stemming. We will be using lemmatization for our work.</p>\n<h3 id="data-selection-decision">Data Selection Decision</h3>\n<p><em>Wednesday, September 20th</em></p>\n<p>After looking through different sources of data this week and talking with my research mentor, it has been decided that using <a href="https://www.ncbi.nlm.nih.gov/pmc/">PubMed Central</a> as the sole data source should give enough documents to have good results. Furthermore, the data source contains parsed documents in XML format which allows for minimal use of additional parsing techniques from PDF format.</p>\n<h3 id="read-through-papers">Read through papers</h3>\n<p><em>Wednesday, September 13th</em></p>\n<p>After being introduced to subjects by reading research papers, I have a decent understanding of the process that is taken to get from the step of text extraction and input to hypothesis generation (with lack of understanding of some specifics). I will be speaking with Justin some time this week in order to work out some questions I have about the process, but overall seems like a very cool process.</p>\n<p>I am now starting to look through different sources of data. The main and quickest source to find is <a href="https://www.ncbi.nlm.nih.gov/pmc/">PubMed Central</a>, but—given that this should have on the order of a couple of million of articles—this could be enough if no other viable source of data is found.</p>\n<p>I have also been looking into different programs for parsing text from PDF format. ### Beginning of Research</p>\n<p><em>Wednesday, September 6th</em></p>\n<p>Met with Dr. Safro and Dr. Herzog, and Justin Sybrandt from the ACS Lab. Introduced to and talked with Justin about his research and where my additional work will fit in.</p>\n<p>What I will be doing is working on reapplying Topic Modeling and Hypothesis Generation with abstracts and also full-text research papers as data input. This will require:</p>\n<ol type="1">\n<li>Reading through paper on <a href="https://arxiv.org/abs/1702.06176">MOLIERE: Automatic Biomedical Hypothesis Generation System</a> as well as others in order to become introduced to research at large within this particular area.\n<ul>\n<li>Others Include\n<ul>\n<li><a href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-11-492">The structural and content aspects of abstracts versus bodies of full text journal articles are different</a></li>\n<li><a href="https://www.biorxiv.org/content/biorxiv/early/2017/07/11/162099.full.pdf">Text mining of 15 million full-text scientific articles</a></li>\n</ul></li>\n</ul></li>\n<li>Finding of papers in large enough size so that reliable results and valid comparisons can be made.</li>\n<li>Parsing out text from papers and removing things like, equations, tables, image links and references.</li>\n</ol>\n</body>\n</html>\n',
                    route: '/projects/topic_modeling_and_hypothesis_generation'
                },
                {
                    name: 'Multi-Robot Environment',
                    text: '<!DOCTYPE html>\n<html>\n<head>\n  <meta charset="utf-8">\n  <meta name="generator" content="pandoc">\n  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">\n  <title></title>\n  <style type="text/css">code{white-space: pre;}</style>\n  <!--[if lt IE 9]>\n    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>\n  <![endif]-->\n</head>\n<body>\n<h1 id="research-science-internship-dimacs-reu-at-rutgers-university">Research Science Internship @ DIMACS REU at Rutgers University</h1>\n<p>Feel free to download and look at my <a href="../Resources/finalResearchPaper.pdf">research paper</a>.</p>\n<h2 id="discrete-representation-of-multi-agent-environment">Discrete Representation of multi-agent environment</h2>\n<img src="../Resources/DiscreteSmaller.gif" />\n<center>\n<em>Discrete Multi-Robot Environment Simulation</em>\n</center>\n<h3 id="diagram-explanation">Diagram explanation</h3>\n<p>This is a simulation of a multi-robot environment in which basic path planning is implemented. It is done in a discrete case to allow for easier path planning. The grid size and hexagonal shape is chosen so that a disc with radius <strong>r</strong> will not collide with another disc of radius <strong>r</strong>.</p>\n<p>Pseudo code</p>\n<ol type="1">\n<li>A Starting and Ending point on the graph are randomly chosen for each disc.</li>\n<li>A breadth first search is done from each discs goal node, that is saved on the node data structure</li>\n<li>On each discrete transition, every node tries to move closer to its goal node, while trying not to run into another disc, using info on the nodes</li>\n<li>Random priorities are decided among the discs and the next step is computed.</li>\n<li>Repeat step 4 until all nodes are at their respective goal states.</li>\n</ol>\n<h2 id="work-summary-and-blog">Work Summary And Blog</h2>\n<p>For this project, I was responsible for generating simulations for multi-robot environments. These simulations would be used in order to determine if there are any sharp-transitions in the behavior of multi-robot systems when the constraints of the system changed, specifically the locality and density of robots in the system.</p>\n<p>Overall, I looked at three different scenarios under these constraints: Continuous, Discrete, and Discrete with Collision avoidance. Often times problems that have to do with a continuous amount of possibilities is simplified to a discrete case to stop it from being able to be solved in a reasonable amount of time, so that was why continuous with path planning was not done.</p>\n<p>In the end, it was determined that there were no sharp transitions in the specific environments and scenarios that I explored. Further research can be done to see what types of properties would lead to such transitions, but as of now my simulations and path determination algorithm do not create sharp transitions.</p>\n<figure>\n<img src="../Resources/ContinuousSmaller.gif" />\n</figure>\n<center>\n<em>Continuous Case</em>\n</center>\n<h2 id="final-week-and-a-research-paper">Final week and a Research Paper</h2>\n<p><em>Week 9</em></p>\n<p>This week, I am continuing to write my final research paper based upon the results that were found. I would like to thank DIMACS and NSF for the funding that I received and also for the great Summer of Research that I had; it was an unforgettable experience.</p>\n<h2 id="final-data-collection">Final data collection</h2>\n<p><em>Week 8</em></p>\n<p>This week, I am collecting more data. Also, working on writing the final research paper and the summary of my experience in the program.</p>\n<h2 id="lack-of-a-sharp-transition">Lack of a sharp transition</h2>\n<p><em>Week 7</em></p>\n<p>This week, I worked more with locality and discovered moreso how it affects the number of collisions overall. There were no unexpected results however and there was also no perceiveable sharp transition. So far it seems like I would have to increase the sophistication for the collision avoidance to get a sharp transition.</p>\n<h2 id="locality-and-disc-radius">Locality and disc radius</h2>\n<p><em>Week 6</em></p>\n<p>This week, I worked more on data collection and exploring the solution. I implemented the locality based on edge distance generation for starting and ending nodes for the discs. I also collected data based on the locality and total discs in the environment as without the locality the results were predictable and there was no sharp transition.</p>\n<h2 id="more-data-and-more-deterministic">More Data and more deterministic</h2>\n<p><em>Week 5</em></p>\n<p>I gathered basic data on the collision behavior, but the more random method of determining solutions to the problem caused a few problems with the speed of solving and thus the speed of collecting data. I worked this week on making the process deterministic. I also collected some more data.</p>\n<h2 id="trying-out-the-gpu">Trying out the GPU</h2>\n<p><em>Week 4</em></p>\n<p>This week, I generated graphs based on how the locality of travel and density of the discs area in a unit square affect the number of collisions. Using plot.ly along with python, I was able to run simulations on the number of pairwise collisions. In the system there appeared to be no sharp transitions.</p>\n<h2 id="i-now-have-graphs">I now have graphs</h2>\n<p><em>Week 3</em></p>\n<p>This week, I generated graphs based on how the locality of travel and density of the discs area in a unit square affect the number of collisions. Using plot.ly along with python, I was able to run simulations on the number of pairwise collisions. In the system there appeared to be no sharp transitions.</p>\n<h2 id="onwards">Onwards!</h2>\n<p><em>Week 2</em></p>\n<p>This week, I ran simulations to determine how the properties of the system affected the interaction between the discs. The results of this step are so far inconclusive since no sharp transition was found yet. I set up python and pygame within visual studios and then started to work in/learn python to create a grid for the discs to be on. This will be useful later for creating models with pygame.</p>\n<h2 id="start-of-the-program">Start of the Program</h2>\n<p><em>Week 1</em></p>\n<p>This week I worked on putting together a presentation as well as making some headway into gathering data on how the number of discs, the radius, and maximum distance from starting point to ending point affect the number of pairwise collisions that occur. I layed out a plan for what I am going to be researching as well as getting my workspace organized and set up for working. I met with my mentor, Jingjin Yu, and we layed out more-so what I will be starting with.</p>\n</body>\n</html>\n',
                    route: '/projects/multi-robot_environment'
                },
                {
                    name: 'Pubsub Architecture Analysis',
                    text: '<!DOCTYPE html>\n<html>\n<head>\n  <meta charset="utf-8">\n  <meta name="generator" content="pandoc">\n  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">\n  <title></title>\n  <style type="text/css">code{white-space: pre;}</style>\n  <!--[if lt IE 9]>\n    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>\n  <![endif]-->\n</head>\n<body>\n<h1 id="research-with-human-centered-cloud-robotics-group-clemson">Research with Human-Centered Cloud Robotics group @ Clemson</h1>\n<h2 id="simulation-of-networks-effect-on-real-time-environemnt">Simulation of network\'s effect on real-time environemnt</h2>\n<p><img src="../Resources/pubSubRep.png" /> ### Tools for Analysis</p>\n<p><em>Tuesday, November 15th, 2016</em></p>\n<p>I mentioned previously that I will be looking at how well different publish and subscribe architectures perform in real time environments. The specific environment that I am looking at is one in which a controller will be controlling, through a broker and a plant, a tiltable plate with a ball on it. this allows for us to analyze the behavior over time and how well it actually performs the task, without any predictive behaviors. The following figures are the basis of my visual analysis, since the ball-plate model is a simulation in the first place. There are really just two things that I am looking at right now, how far the ball is from its desired location in the x and y direction and a histogram of how long the sending of data actually takes.</p>\n<figure>\n<img src="../Resources/figure_1.png" />\n</figure>\n<center>\n<em>Distribution of Time Taken</em>\n</center>\n<p><img src="../Resources/figure_2.png" /> <img src="../Resources/figure_3.png" /></p>\n<center>\n<em>How Far from Path</em>\n</center>\n<p>These diagrams will generally help with basic analysis, but numerical analysis and comparisons will be done as well.</p>\n<h3 id="mininet">Mininet</h3>\n<p><em>Wednesday, November 2nd, 2016</em></p>\n<p>Over the past few weeks, I have been working on creating network topologies with Mininet[1], then testing the performance of MQTT QoS 2 in order to run a ball-plate simulation. A ball-plate simulation is one in which we literally simulate three entities: a ball being balanced on plate, an observer (plant), and a controller. The observer desires the ball to be in a certain location on the plate and also in constant motion. The controller just listens the position that the observer wants and tries to make it happen if it can.</p>\n<p>Now the specific network topology only consists of 3 hosts and their respective switches; the hosts are called Broker, Plant, and Controller. The topology is shown in the diagram below.</p>\n<figure>\n<img src="../Resources/diagram1.png" />\n</figure>\n<center>\n<em>Network Topology</em>\n</center>\n<p>Although a basic topology, it will be perfect for simple analysis of the effect of latency and time delay between transmissions for the data between the host, broker, and plant. Originally there were some problems with creating the network topology, because I was not running a controller on my machine.</p>\n<h3 id="simulating-network-topology-with-mininet">Simulating Network Topology with Mininet</h3>\n<p><em>Wednesday, October 12th, 2016</em></p>\n<p>I have also been looking into some of the features of Mininet, which is an application (or an extension of Linux) that allows you to construct a network and create connections between virtual hosts and servers within a network. Actually, the specific structure that I am wanting to create is one in which there is a publisher, a subscriber, and a broker or a controller, plant, and broker. After I achieve that, I will be adding more and more subscribers and/or publishers and then study the amount of traffic that goes through the network and how real time tasks are affected by the changes on the load within the server and Network. I will then be adding Kafka and MQTT into the mix and comparing the two. Not only comparing generic MQTT and Kafka, but the three levels of MQTT (explained below) and Kafka. Right now, I am currently working more and more with Mininet in order to learn how to accomplish these tasks. [3]</p>\n<h3 id="learning-more-about-kafka-and-mqtt">Learning more about Kafka and MQTT</h3>\n<p><em>Tuesday, October 11th, 2016</em></p>\n<p>Over the past week I have been looking more into Kafka and MQTT in order to get a better understanding of their purposes and drawbacks. I spoke with my mentor, Dr. Remy, and looked at some of the drawbacks and benefits of these publish and subscribe architecture with respect to the amount of overhead that is associated with each. Overall, it seems that Kafka processes less information in the server and then sends more data in order to achieve the same task on real-time process, whereas MQTT seems to process more within the servers and then send less data overall. Hence the architecture, MQTT, living up to its name of mosquito. MQTT has three levels of service, unlike Kafka which is always guaranteed at least once delivery; MQTT can be at most once but maybe not at all (QoS 0), at least once (QoS 1), and at most once (QoS 2) whereas Kafka is usually only guaranteeing at least once delivery. This creates the possibility for a lot of data being sent over the network, but is also quicker than at most once delivery, because of the lack of a 3 way handshake. [1, 2] I am curious about what the size of the Kafka messages were that were stored on the server, because if the link to the next message is just as long as the message, then we will definitely get a much larger amount of data sent. Overall, it seems that the larger the message being sent in Kafka, the more the overhead shrinks in comparison; in other words, the header does not need to change size when the message size changes by that much so the theoretically optimal thing to do is to have a message of infinite length and then the overhead would be zero. (not really feasible but makes mathematical sense).</p>\n<h3 id="learning-more-about-tcp">Learning More about TCP</h3>\n<p><em>Monday, October 10th, 2016</em></p>\n<p>Last week I worked on our Networking class project for TCP. I started out with just a basic skeleton file with the basic necessities of establishing a TCP connection between an echo server and an echo client. I made a basic overarching protocol where header information was sent first with the amount of bytes that were going to be sent, and the number of messages that would be sent. With this, I could declare if there were going to be more messages and how many more would be sent. This way there would be no way for the server to close the connection before the client was done communicating with the server. The server then did the simple operation of just inverting the characters in the array; it just switched the case of the letters. This was a toy project just to learn how to use TCP and how to guarantee that the connection does not close before the message is done sending.</p>\n<h3 id="dweepy-and-dweet.io-analysis">Dweepy and Dweet.io Analysis</h3>\n<p><em>Tuesday, September 6th, 2016</em></p>\n<p>This past week I worked on creating software for in depth analysis of the sending and receiving procedure for dweet.io! I started with creating software to time how long it takes for a message to be published and a message to be received when using dweepy. Then, using tcpdump, I analyzed how many messages were sent and received. Using pexpect, a python library for scripting, I ran tcpdump as a child process and used it to analyze things like, how many sends, time between sends and receives, the number of sends and receives, the number of bytes sent, and finally the average ordering of the send receive procedures as well as the percentage of n messages being sent over the trials.</p>\n<p>After collecting the data, we can analyze the amount of overhead that exists for sending an integer (4 bytes) and a string of length 10 (10 bytes) for a total of 14 bytes. For both publishing and subscribing, we receive 5,500 bytes and send 1,500 bytes. Also, I looked at the time it takes to get from the dweet.io to the first requests, which doesn\'t seem to be that long on average. Most of the time was spent during the send and receives between the server and client; there were 16 send/receives in totality.</p>\n<p>One problem with my method was that I was waiting 1 second with pexpect in order to see if anymore messages would be sent and received, but this gave the possibility for outliers to be thrown into the mix which resulted in some strange time problems. (Negative time results) Overall, the quality of the results so far is best when looking at the average ordering and the number of bytes sent. In order to improve, I will rerun the trials with a longer delay in between functions in order to catch the outliers that take a long time to send a response.</p>\n<p>It is interesting that so much data is actually sent to publish such a small message, but further research must be done to see what can be removed from the process. I will start working with pycurl and implement my own method for communicating with the server and see how that affects the overall times.</p>\n</body>\n</html>\n',
                    route: '/projects/pubsub_architecture_analysis'
                },
                {
                    name: 'Building Websites',
                    text: '<!DOCTYPE html>\n<html>\n<head>\n  <meta charset="utf-8">\n  <meta name="generator" content="pandoc">\n  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">\n  <title></title>\n  <style type="text/css">code{white-space: pre;}</style>\n  <!--[if lt IE 9]>\n    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>\n  <![endif]-->\n</head>\n<body>\n<h1 id="my-website">My Website</h1>\n<p><strong>Built with react, autogenerated from markdown documents and directory structure, with bootstrap layered on top of it</strong></p>\n<p>This website is compiled and then statically served, which makes it good for use on any web server. I know now that there is <a href="https://gohugo.io/">Hugo</a>, but I enjoyed the process of putting all this together.</p>\n<p>All the html that is presented before you, is generated directly from the directory layout of files. Directories are turned into drop-down menus. (This does however become difficult when it is a multi-level drop-down, which I plan to have if time allows)</p>\n<p>Code is on <a href="https://github.com/acarrab/acarrab.github.io">Github</a></p>\n<h2 id="there-was-also-my-other-website">There was also my other website</h2>\n<p>This <a href="/OldWebsite">older website</a> was built for fun and from a low level. I think it is really fun to travel around though! I made <strong><em>interesting</em></strong> design choices though...</p>\n<p>I would have kept using this, if it wasn\'t for the need to have good mobile support.</p>\n<p><a href="/OldWebsite"><img src="../Resources/OldWebsiteScreenCap.png" /></a></p>\n</body>\n</html>\n',
                    route: '/projects/building_websites'
                },
                {
                    name: 'Deep Learning',
                    text: '<!DOCTYPE html>\n<html>\n<head>\n  <meta charset="utf-8">\n  <meta name="generator" content="pandoc">\n  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">\n  <title></title>\n  <style type="text/css">code{white-space: pre;}</style>\n  <!--[if lt IE 9]>\n    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>\n  <![endif]-->\n</head>\n<body>\n<h1 id="deep-learning-projects">Deep learning Projects</h1>\n<p>Right now, I am working on a handful of projects that pertain to deep learning as well as making my way through <a href="http://www.deeplearningbook.org/">this book</a>. This started from long-running interest in machine learning. After taking some of the math classes that pertain to deep learning (Linear Algebra, Discrete Mathematics, Theory of Probability, Differential Equations, and Multi-variable Calculus), I want to learn deep learning in <em>depth</em>.</p>\n<h2 id="senior-capstone-project">Senior Capstone Project</h2>\n<p>Currently working on a senior capstone project in which we have been entrusted 2 <strong>NVIDIA Tesla P100 GPUs</strong>. These are GPUs that are specifically made to tackle some of the biggest problems in Machine Learning. Our group has been working on creating the proper infrastructure that will be able to efficiently and without additional bottlenecks.</p>\n<p>I specifically have been researching different hardware requirements for the GPUs, working on finding some good examples that showcase their powerful performance. So far, our group--through one of our clemson faculty members--is ordering components, like a server rack and other necessities.</p>\n<h3 id="goals-for-the-project">Goals for the project</h3>\n<p>We are working on a handful of goals, but here are the basics</p>\n<ol type="1">\n<li>Setting up Tesla P100 Architecture</li>\n<li>Take online classes in order to learn more about deep-learning frameworks</li>\n<li>Implement some projects within the system after setup</li>\n<li>Benchmark the performance of the machine against other machines in the department</li>\n<li>Create tutorial tailored for students in order to give them familiarity with the system</li>\n<li>Create fun example in order to showcase one of many cool projects within the Computer Science Department at Clemson</li>\n</ol>\n<h2 id="technical-writing">Technical Writing</h2>\n<p>(Yes even within Technical Writing)</p>\n<p>Our class is making a showcase and giving a presentation of the new technologies of that are available through new IoT/Big Data. Beyond that, a large aspect of the project is our groups teaming up with groups at University of Braunschweig in Germany.</p>\n<p>Our group, on the other hand, is taking this opportunity to showcase real-time Object-Recognition and Classification using some neat libraries. (However, we have not yet decided between Tensor Flow, DIGITS, or Watson\'s object recognition software)</p>\n<h2 id="personal-goals">Personal Goals</h2>\n<p>I am working on learning more of the math in order to be able to create tailored machine learning models in order to tackle real-world problems. However, I do not want to use something so cool and not really know what is going on underneath the hood.</p>\n<p>My routines for learning consist of</p>\n<ul>\n<li>Reading through <a href="http://www.deeplearningbook.org/">this textbook</a></li>\n<li>Watching <a href="https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A">Siraj</a></li>\n<li>Trying to get into a course on <a href="https://www.udacity.com/">Udacity</a> for deep learning</li>\n</ul>\n</body>\n</html>\n',
                    route: '/projects/deep_learning'
                }
            ],
            children: [
            ]
        },
        {
            title: 'School Work',
            pages: [
                {
                    name: '2015 Spring | 102 Raytracer',
                    text: '<!DOCTYPE html>\n<html>\n<head>\n  <meta charset="utf-8">\n  <meta name="generator" content="pandoc">\n  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">\n  <title></title>\n  <style type="text/css">code{white-space: pre;}</style>\n  <!--[if lt IE 9]>\n    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>\n  <![endif]-->\n</head>\n<body>\n<h1 id="ray-tracer">Ray Tracer</h1>\n<p><em>Spring 2015</em></p>\n<p>Wrote ray-tracer code in order to render a spaceship. The core aspects of the class were to be introduced to c++, but using additional knowledge gained through classes like Multi-Variable calculus, I was able to create unique 3d structures. Also implemented texture mapping to a sphere, which is showcased in the photo.</p>\n<h3 id="result">Result</h3>\n<figure>\n<img src="../Resources/SpaceRaytracer.jpg" />\n</figure>\n</body>\n</html>\n',
                    route: '/school_work/2015_Spring_102_RayTracer'
                },
                {
                    name: '2017 Spring | Computer Graphics',
                    text: '<!DOCTYPE html>\n<html>\n<head>\n  <meta charset="utf-8">\n  <meta name="generator" content="pandoc">\n  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">\n  <title></title>\n  <style type="text/css">code{white-space: pre;}</style>\n  <!--[if lt IE 9]>\n    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>\n  <![endif]-->\n</head>\n<body>\n<h1 id="computer-graphics-projects">Computer Graphics Projects</h1>\n<h2 id="teapot-with-rendering-equation-approximation">Teapot with Rendering Equation approximation</h2>\n<p><em>Late Spring 2016</em></p>\n<p>This project was done in order to approximate the light scattering effects that occur within real-world environments.</p>\n<p>Using <a href="http://graphics.stanford.edu/courses/cs348b-10/lectures/renderingequation/renderingequation.pdf">Radiosity equations</a> and randomized sampling around spheres according approximate things like light scatter as well.</p>\n<h3 id="result">Result</h3>\n<figure>\n<img src="../Resources/renderingEquationTeapot.png" />\n</figure>\n<h2 id="stanford-bunny">Stanford Bunny</h2>\n<p><em>Early Spring 2016</em></p>\n<p>For this project, our group worked with the classic stanford bunny and applied some of our newly learned knowledge in computer graphics to generate a model of the bunny and make it look pretty.</p>\n<p>In hindsight, the motion blur was too much :(</p>\n<figure>\n<img src="../Resources/stanfordBunny.png" />\n</figure>\n</body>\n</html>\n',
                    route: '/school_work/2017_Spring_Computer_Graphics'
                },
                {
                    name: '2017 Spring | 2D Game Engine Design',
                    text: '<!DOCTYPE html>\n<html>\n<head>\n  <meta charset="utf-8">\n  <meta name="generator" content="pandoc">\n  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">\n  <title></title>\n  <style type="text/css">code{white-space: pre;}</style>\n  <!--[if lt IE 9]>\n    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>\n  <![endif]-->\n</head>\n<body>\n<h1 id="d-game-engine-design">2D Game Engine Design</h1>\n<p><em>Spring 2017</em></p>\n<p>This was a class intending on improving object oriented code quality and c++ understanding. All students worked on a semester long project with the end result of having a game being built and functional. For this task, the only tool we used other than stl was the <a href="https://www.libsdl.org/">SDL (Simple Direct Media Layer) 2.0</a> framework.</p>\n<h2 id="my-game">My game</h2>\n<p>I wanted to do something where I incorporated fun and amusing particle physics within the game. What better way to do this than to do this with a particle wizard! Using quick collision checking methods and some design patterns to improve things like speed and memory efficiency, the game is able to support the firing and handling of many particles in the system.</p>\n<video width="640" height="480" controls>\n<source src="../Resources/WizardGame.mp4" type="video/mp4">\n</source>\n</video>\n</body>\n</html>\n',
                    route: '/school_work/2017_Spring_2D_Game_Engine_Design'
                },
                {
                    name: 'Fun Courses',
                    text: '<!DOCTYPE html>\n<html>\n<head>\n  <meta charset="utf-8">\n  <meta name="generator" content="pandoc">\n  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">\n  <title></title>\n  <style type="text/css">code{white-space: pre;}</style>\n  <!--[if lt IE 9]>\n    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>\n  <![endif]-->\n</head>\n<body>\n<h1 id="funrecommended-clemson-courses-to-take">Fun/Recommended Clemson Courses to take</h1>\n<p>These are classes I have taken out of interest or thought would help me with my academic goals.</p>\n<h3 id="fall-2017">Fall 2017</h3>\n<ul>\n<li>Senior Capstone Project 4910</li>\n<li>Focusing on learning deep learning concepts and some infrastructure specifics</li>\n<li>Programming Systems</li>\n<li>The <em>how it\'s made</em> of programming languages</li>\n</ul>\n<h3 id="spring-2017">Spring 2017</h3>\n<ul>\n<li>Design and Analysis of Algorithms 8400</li>\n<li>Learned advanced runtime analysis and better application of known algorithms</li>\n<li>Data Science</li>\n<li>Learned high level information about statistical models</li>\n<li>Computer Graphics</li>\n<li>Fun geometry and calculus as well as well as the need to work in highly constrained environment</li>\n</ul>\n<h3 id="fall-2016">Fall 2016</h3>\n<ul>\n<li>Networks and Network Programming</li>\n<li>Worked on Honors research (with Human-Centered Cloud Robotics) while in this class</li>\n</ul>\n<h3 id="spring-2016">Spring 2016</h3>\n<ul>\n<li>Ordinary Differential Equations (Honors Version)</li>\n<li>Linear Algebra</li>\n</ul>\n<h3 id="fall-2015">Fall 2015</h3>\n<ul>\n<li>Algorithms and Data Structures</li>\n<li>Still in top 5 favorite classes</li>\n<li>Organic Chemistry</li>\n<li>Fun class that teaches students to work with abstract, feature-described, objects and try to predict their interactions.</li>\n</ul>\n<h3 id="spring-2015">Spring 2015</h3>\n<ul>\n<li>Calculus of Several Variables</li>\n<li>Comp Sci 102</li>\n<li>First project heavy computer science course</li>\n<li>This is where I made <a href="#/school_work/2015_Spring_%7C_102_RayTracer">the ray-tracer</a> and code for modeling the scene</li>\n</ul>\n</body>\n</html>\n',
                    route: '/school_work/fun_courses'
                }
            ],
            children: [
            ]
        }
    ]
}

export function CompiledRoutes() {
    return (
        <Switch>
            <Route exact path='/' component={() => (<div dangerouslySetInnerHTML={{ __html: '<!DOCTYPE html>\n<html>\n<head>\n  <meta charset="utf-8">\n  <meta name="generator" content="pandoc">\n  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">\n  <title></title>\n  <style type="text/css">code{white-space: pre;}</style>\n  <!--[if lt IE 9]>\n    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>\n  <![endif]-->\n</head>\n<body>\n<h1>\nAngelo Carrabba<br /><small style="color: #FF6500; font-weight: 200">Clemson Computer Science Senior</small>\n</h1>\n<h2 id="here-are-some-things-you-can-do-here">Here are some things you can do here</h2>\n<ol type="1">\n<li>Please check out my <a href="#/about">about page</a>, it has some information and project history.</li>\n<li>If you are bored, you can visit my <a href="../Resume/Resume.pdf">Resume</a>.</li>\n<li>If you are really bored, you can look at my <a href="https://github.com/acarrab">GitHub</a>.</li>\n<li>If you are extremely bored, you can browse <a href="https://imgur.com/r/FunnyAnimals">this</a>.</li>\n</ol>\n<h2 id="current-work">Current Work</h2>\n<h3 id="research">Research</h3>\n<ul>\n<li><a href="#/projects/topic_modeling_and_hypothesis_generation">Topic Modeling and Hypothesis Generation (Data Mining Research)</a></li>\n</ul>\n<h3 id="personalacademic">Personal/Academic</h3>\n<ul>\n<li>Learning more about different statistical machine learning processes, particularly deep-learning</li>\n<li><a href="#/projects/deep_learning">more info</a></li>\n</ul>\n<h3 id="past-work">Past Work</h3>\n<ul>\n<li><a href="#/projects/multi-robot_environment">Multi-agent Environment Research (REU)</a></li>\n<li><a href="#/projects/pubsub_architecture_analysis">PubSub Architecture Analysis</a></li>\n</ul>\n</body>\n</html>\n'}} />)} />
            <Route path='/about' component={() => (<div dangerouslySetInnerHTML={{ __html: '<!DOCTYPE html>\n<html>\n<head>\n  <meta charset="utf-8">\n  <meta name="generator" content="pandoc">\n  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">\n  <title></title>\n  <style type="text/css">code{white-space: pre;}</style>\n  <!--[if lt IE 9]>\n    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>\n  <![endif]-->\n</head>\n<body>\n<h1 id="about-me">About me</h1>\n<p>Hello people, this page is coming to you live from a server somewhere on the interweb!</p>\n<p>Anyways, my name is Angelo and I am a Senior Computer Science Student at Clemson. I do things and some of those things are, well, on this website.</p>\n<h2 id="interests">Interests</h2>\n<p>I started out at Clemson University on a pre-med and computer science track; However, saw the impact that was possible by going with a more computer science oriented approach. Since then, I have worked on and helped with a handful of medically based research projects. I plan on going to graduate school and applying/developing my skills in order to contribute to the progression of the medical field.</p>\n<p>I enjoy working with algorithm design, machine learning, and big data.</p>\n<p><a href="/OldWebsite">Websites</a> and other computer graphics related applications are fun as well.</p>\n<h2 id="projects">Projects</h2>\n<h3 id="honors-thesis-work">Honors Thesis Work</h3>\n<p><em>2017-2018 (present)</em></p>\n<p><strong><a href="#/projects/topic_modeling_and_hypothesis_generation">MORE INFO</a></strong></p>\n<p>Year long project with goal of improving existing techniques for hypothesis generation by extracting relevant information from research papers.</p>\n<p>I am working with big data (1.7 million Medical Research Articles from <a href="https://www.ncbi.nlm.nih.gov/pmc/">PubMed Central</a>)</p>\n<h3 id="software-engineering-intern-at-blackbaud">Software engineering intern at <a href="https://www.blackbaud.com/">Blackbaud</a></h3>\n<p><em>Summer 2017</em></p>\n<figure>\n<img src="../Resources/blackbaud.jpg" />\n</figure>\n<center>\n<em>That is me kneeling at the bottom right</em>\n</center>\n<p>Blackbaud is a company that creates software tailored for nonprofit organizations. The most used product is Raiser\'s Edge, which is used by nonprofits when determining how to most efficiently handle their donors.</p>\n<p>Worked on a Scrum team within an ASP.net web application called Raisers Edge NXT—which makes up about 30% of their sales—with Angular 2 on the front-end and C# and SQL on the back-end. Updated and added health pages that gave developers the tools to better analyze database health and edit database package status. Reduced the number of health pages by making a more centralized location for health data.</p>\n<p>Specifically, wrote JavaScript with Angular 2 Components and Services. As well as JavaScript Jasmine unit tests. On the back-end wrote in C# with SpecFlow unit tests and integration tests. Wrote basic SQL as well.</p>\n<h3 id="research-with-human-centered-cloud-robotics-clemson-university">Research with Human-Centered Cloud Robotics @ Clemson University</h3>\n<p><em>Aug. 2016 - Jan. 2017</em></p>\n<p><strong><a href="#/projects/pubsub_architecture_analysis">MORE INFO</a></strong></p>\n<ul>\n<li>Analyzed pubsub architectures performance in real-time environments, or simulations of real-time environments with no prediction.</li>\n<li>Simulations done in Mininet with python scripts for generating the network topology.</li>\n<li>Looked specifically at MQTT and Kafka</li>\n</ul>\n<h3 id="research-at-rutgers-university-with-dimacs">Research at Rutgers University with <a href="http://dimacs.rutgers.edu/">DIMACS</a></h3>\n<p><em>Summer 2016</em></p>\n<p><strong><a href="#/projects/multi-robot_environment">MORE INFO</a></strong></p>\n<p>Joined research experience for Undergrads program at Rutgers University.</p>\n<ol type="1">\n<li>Wrote a <a href="../Resources/finalResearchPaper.pdf">research paper</a> on results.</li>\n<li>Created a simulation in python, with graphics created with pygame.</li>\n<li>Analyze traffic in a multi-robot environment with multi-agent simulations.</li>\n<li>Specifically looking for sharp-transition in behavior in discretized traffic network when compared to continuous one.</li>\n</ol>\n<h4 id="takeaway">Takeaway</h4>\n<p>Determined through self-created simulations that there were no sharp transition in traffic, based on changes in robot size and start-end locality.</p>\n<h3 id="applied-algorithms-lab-clemson-university">Applied Algorithms Lab @ Clemson University</h3>\n<p><em>Jan 2016 - June 2016</em></p>\n<p>Implemented DTW Search with modifications to improve result quality with noisy signal. For EEG database to add search functionality for Neurologists at MUSC research hospital.</p>\n<h3 id="virtual-environment-group-clemson-university">Virtual Environment Group @ Clemson University</h3>\n<p><em>Sep. 2014 - Aug. 2015</em></p>\n<p>Research Assistant C Designed computer vision software in C, OpenGL, and GLUT. Worked on software for calibrating camera image distortion through analysis of an image of a checkered board. Created edge detection processing.</p>\n</body>\n</html>\n'}} />)} />
            <Route path='/projects/topic_modeling_and_hypothesis_generation' component={() => (<div dangerouslySetInnerHTML={{ __html: '<!DOCTYPE html>\n<html>\n<head>\n  <meta charset="utf-8">\n  <meta name="generator" content="pandoc">\n  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">\n  <title></title>\n  <style type="text/css">code{white-space: pre;}</style>\n  <style type="text/css">\ndiv.sourceCode { overflow-x: auto; }\ntable.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {\n  margin: 0; padding: 0; vertical-align: baseline; border: none; }\ntable.sourceCode { width: 100%; line-height: 100%; }\ntd.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }\ntd.sourceCode { padding-left: 5px; }\ncode > span.kw { color: #007020; font-weight: bold; } /* Keyword */\ncode > span.dt { color: #902000; } /* DataType */\ncode > span.dv { color: #40a070; } /* DecVal */\ncode > span.bn { color: #40a070; } /* BaseN */\ncode > span.fl { color: #40a070; } /* Float */\ncode > span.ch { color: #4070a0; } /* Char */\ncode > span.st { color: #4070a0; } /* String */\ncode > span.co { color: #60a0b0; font-style: italic; } /* Comment */\ncode > span.ot { color: #007020; } /* Other */\ncode > span.al { color: #ff0000; font-weight: bold; } /* Alert */\ncode > span.fu { color: #06287e; } /* Function */\ncode > span.er { color: #ff0000; font-weight: bold; } /* Error */\ncode > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */\ncode > span.cn { color: #880000; } /* Constant */\ncode > span.sc { color: #4070a0; } /* SpecialChar */\ncode > span.vs { color: #4070a0; } /* VerbatimString */\ncode > span.ss { color: #bb6688; } /* SpecialString */\ncode > span.im { } /* Import */\ncode > span.va { color: #19177c; } /* Variable */\ncode > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */\ncode > span.op { color: #666666; } /* Operator */\ncode > span.bu { } /* BuiltIn */\ncode > span.ex { } /* Extension */\ncode > span.pp { color: #bc7a00; } /* Preprocessor */\ncode > span.at { color: #7d9029; } /* Attribute */\ncode > span.do { color: #ba2121; font-style: italic; } /* Documentation */\ncode > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */\ncode > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */\ncode > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */\n  </style>\n  <!--[if lt IE 9]>\n    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>\n  <![endif]-->\n</head>\n<body>\n<h1 id="honors-research-project-for-senior-year">Honors Research Project for Senior Year</h1>\n<p>This project is a school year long project in which I am working along side grad students in the ACS Lab (Algorithms and Computational Sciences Lab). I am adding to the work being done by <a href="http://sybrandt.com/">Justin Sybrandt</a> by determining if we can increase the accuracy of Moliere, a hypothesis generation method.</p>\n<p>We are seeing if it helps to extract additional information from the full-texts and, if so, how to best do so.</p>\n<h2 id="progress">Progress</h2>\n<p><em>Last updated: Tuesday, October 30th 2017</em></p>\n<p>All code is parallelized and ran on the <a href="https://www.palmetto.clemson.edu/palmetto/userguide_palmetto_overview.html">Clemson Palmetto Cluster</a> in order to complete tasks in reasonable amount of time. Using <a href="http://mpi4py.readthedocs.io/en/stable/">mpi4py</a> in order to run text extraction on the million documents in parallel on the Clemson Palmetto Cluster</p>\n<p>Started by downloading 1.7 million documents over ftp from <a href="https://www.ncbi.nlm.nih.gov/pmc/">PubMed Central</a></p>\n<ol type="1">\n<li>Wrote code to parse xml documents tree structure with hierarchical selection by walking through xml tree and grabbing what I need (example under <strong>hierarchical selection example</strong> section)\n<ul>\n<li>Down to 1.4 million documents because: some did not have abstracts, were not research papers, or were not in proper xml format.</li>\n</ul></li>\n<li>Wrote and ran code to clean text of unicode expressions and in text equations using regular expressions.</li>\n<li>Used <a href="http://www.nltk.org/">NLTK</a> in order to lemmatize text in order to be passed into the next part of the pipeline, the creation of the n_grams.</li>\n<li>Ran Abstracts through n-gram stage of Moliere Pipeline</li>\n</ol>\n<h2 id="weekly-log">Weekly Log</h2>\n<h3 id="n-gram-creation-using-topmine">N-gram creation using <a href="https://arxiv.org/pdf/1406.6312.pdf">ToPMine</a></h3>\n<p><em>Monday, October 30th</em></p>\n<p>Learned how create n-grams through ToPMine with help of Justin, then ran the process on Abstracts document subset.</p>\n<p>Currently only on Abstract documents and analyzed the validity of the results through randomly looking at generated topics by looking through topics and seeing if topics made sense. All of them, to me, seem to be valid pairings of things that occur in the same statement so I will be moving ahead with running n-gram creation on a subset of the full-texts now.</p>\n<p>Here are some of the topics that were found.</p>\n<pre><code>significantly reduced\ndata were collected\nwa examined\namino acid\nincreased risk\nwa present\ncell type\ncell grown\nclosed loop\ncomplex structure\nset of gene\ntechnique based\nhigh dose\nsurvival rate\nhealth outcome\nhuman cell</code></pre>\n<h4 id="problem-with-loss-of-lines">Problem with loss of lines</h4>\n<p>Some of the abstracts were parsed out entirely, so lines were removed, which made us lose tracking information of the document id. There were about 50 that contained all unique words and we can no longer map back the lines since they are different. In order to resolve this issue, it is required to go back and append a non-unique sentence that will not affect the lines.</p>\n<p>It was recommended to fix this problem by appending <code>the quick brown fox.</code> to each document which are placed on separate lines, which was easy to do with sed via a simple bash script</p>\n<p><strong>quickBrownFoxify.sh</strong></p>\n<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="co">#!/bin/bash</span>\n\n<span class="va">newFile=</span>qbf_<span class="va">$1</span>\n<span class="fu">cp</span> <span class="va">$1</span> <span class="va">$newFile</span>\n<span class="fu">sed</span> -i -e <span class="st">&#39;s/^/Quick Brown Fox. /&#39;</span> <span class="va">$newFile</span></code></pre></div>\n<p>which is called by</p>\n<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="ex">./quickBrownFoxify.sh</span> allAbstracts.txt</code></pre></div>\n<p>Now It must be ran again to create the n-grams.</p>\n<h4 id="creating-random-subset-of-documents-from-full-texts-for-testing">Creating random subset of documents from full-texts for testing</h4>\n<p>Each document is given a random and equal chance of selection while walking through file for a total of 100,000 randomly selected documents.</p>\n<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> random\nn <span class="op">=</span> <span class="dv">1328035</span>\nk <span class="op">=</span> <span class="dv">100000</span>\n\n<span class="cf">with</span> <span class="bu">open</span>(<span class="st">&quot;fullTexts_subset.txt&quot;</span>, <span class="st">&quot;w&quot;</span>) <span class="im">as</span> fout:\n    <span class="cf">with</span> <span class="bu">open</span>(<span class="st">&quot;allFulltexts.txt&quot;</span>, <span class="st">&quot;r&quot;</span>) <span class="im">as</span> fin:\n        <span class="cf">for</span> document <span class="kw">in</span> fin:\n            <span class="co"># gives us proper chance of selecting document</span>\n            <span class="cf">if</span> <span class="dv">0</span> <span class="op">==</span> random.randrange(<span class="dv">0</span>, <span class="bu">int</span>(n <span class="op">/</span> k)) <span class="kw">or</span> n <span class="op">==</span> k:\n                fout.write(document)\n                k <span class="op">-=</span> <span class="dv">1</span>\n                <span class="cf">if</span> k <span class="op">%</span> <span class="dv">10000</span> <span class="op">==</span> <span class="dv">0</span>: <span class="bu">print</span>(<span class="st">&quot;written: &quot;</span> <span class="op">+</span> <span class="bu">str</span>(<span class="dv">100000</span> <span class="op">-</span> k))\n                <span class="cf">if</span> k <span class="op">==</span> <span class="dv">0</span>: <span class="cf">break</span>\n            n <span class="op">-=</span> <span class="dv">1</span></code></pre></div>\n<h3 id="removal-of-files-that-are-too-new-from-the-dataset">Removal of files that are too new from the dataset</h3>\n<p><em>Tuesday, October 24th</em></p>\n<p>Went back through pipeline that has been created so far and added a small amount of code to determine if the publish date of the article was this year as we will be removing those from the data set in order to see if prediction is valid for this year. No need for predicting things we really don\'t know yet, because there is no basis for comparison.</p>\n<p>At the end of this process, it has been determined that there are now only 1.3 million articles left in totality. There are a suspicious amount of articles that are lost to being too new and I will be looking at that later, but for now am just working on lemmatizing the text using different methods.</p>\n<p>Here is the breakdown of documents (that are currently stored in separate files according to the number of processes):</p>\n<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash">[<span class="ex">acarrab@login001</span> lemmatizedText]$ wc -l Abstracts_* <span class="kw">|</span> <span class="fu">grep</span> total\n   <span class="ex">1328035</span> total</code></pre></div>\n<p>Here is the breakdown of how the documents were lost.</p>\n<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash">[<span class="ex">acarrab@login001</span> acarrab]$ wc -l *.log\n  <span class="ex">151427</span> failedDuringParsingOrNoAbstract.log\n   <span class="ex">53872</span> hasAbstractAndFailed.log\n  <span class="ex">164944</span> tooNewButHadAbstract.log\n  <span class="ex">370243</span> total</code></pre></div>\n<p>NOTE: <code>tooNewButHadAbstract.log</code> is not necessarily accurate since it would put 3000 as year if the parsing could not find the date.</p>\n<p>I have looked at lemmatization through the means of using specialist NLP tool in order to use those to lemmatize the text, but to no avail. (A lot more learning than initially thought was needed is needed in order to use their tools).</p>\n<p>In order to get some results, used the NLTK\'s lemmatizer in order lemmatize the text from the research papers. The papers are now ready for the next phase, the building of the n-grams.</p>\n<p>After speaking with Justin, he has seen that the benefits of using the Specialist NLP Tools is actually too slow for our data-set size in the end anyways and NLTK lemmatization does a good job and gives promising and significant results within the research that he has done.</p>\n<h3 id="increasing-valid-parsing-rate">Increasing valid parsing rate</h3>\n<p><em>Monday, October 16th</em></p>\n<p>After running the jobs in order to parse the documents, added basic statistics capturing that are added to files while running batch job on the almost 1.8 million documents. At the end of the run, there are counts for different types of errors</p>\n<ul>\n<li><p>Failure to parse xml errors</p></li>\n<li><p>Had abstract keyword within file, but still failed</p></li>\n<li><p>Failed for other reasons</p></li>\n</ul>\n<p>Given this information, I can now look at the files that failed yet had the abstract keyword and determine how to reduce the number of files that are failed to be parsed.</p>\n<h3 id="quality-analysis-of-xml-parsing">Quality analysis of xml parsing</h3>\n<p><em>Wednesday, October 11th</em></p>\n<p>Went through a random subset of the documents and looked at results from parsing in order to determine whether the parsing was doing what it should be doing. After making some modifications to the code written in the previous week, there is a good likelihood of valid parsing.</p>\n<p>Also inserted failure cases for parse if things like the abstracts were missing.</p>\n<p>Ran parsing parallelized on palmetto cluster and looked determined results so far. More validation should be done in order to make sure that most of the data are being used.</p>\n<p>Files are saved in only abstract format and also full-text and abstract format in order to create distinct sets of documents that can give good comparison.</p>\n<h3 id="heirarchical-parsing-of-xml-documents">Heirarchical parsing of xml documents</h3>\n<p><em>Wednesday, October 4th</em></p>\n<p>Wrote code to parse out unicode characters as well as select out abstracts and other relevant text sections from the millions of documents.</p>\n<p>Created method for parsing out xml documents tags that are needed in with hierarchical selection.</p>\n<h4 id="example-and-explanation">Example and Explanation</h4>\n<p>Wrote code that extracts the following information by populating a data object with the keywords under the <code>&quot;GET&quot;</code> keyword under selection through chains of nested objects. For example, if an article title is found in the xml tree under <code>&lt;article-meta&gt;&lt;title-group&gt;&lt;article-title&gt;</code> tags, then <code>data[&quot;Title&quot;]</code> will be set to an array containing the found data.</p>\n<p>This is a kind of cool way to extract the required information from the xml tree structure. Kind of inspired by <a href="http://graphql.org/">graphql</a> selection statements in order to populate arrays of data.</p>\n<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">chains <span class="op">=</span> [\n    {\n        <span class="st">&quot;article-meta&quot;</span>: {\n            <span class="st">&quot;title-group&quot;</span>: {\n                <span class="st">&quot;article-title&quot;</span>: {\n                    <span class="st">&quot;GET&quot;</span>: <span class="st">&quot;Title&quot;</span>\n                }\n            }\n        },\n        <span class="st">&quot;kwd-group&quot;</span>: {\n            <span class="st">&quot;kwd&quot;</span>: {\n                <span class="st">&quot;GET&quot;</span>: <span class="st">&quot;Keywords&quot;</span>\n            }\n        },\n        <span class="st">&quot;contrib-group&quot;</span>: {\n            <span class="st">&quot;contrib&quot;</span>: {\n                <span class="st">&quot;GETALL&quot;</span>: <span class="st">&quot;Contributors&quot;</span>\n            }\n        },</code></pre></div>\n<h3 id="downloading-documents">Downloading Documents</h3>\n<p><em>Thursday, September 28th</em></p>\n<p>Downloaded the documents over ftp from <a href="https://www.ncbi.nlm.nih.gov/pmc/">PubMed Central</a> and moved them onto the Palmetto Cluster.</p>\n<p>Became familiar with some of <a href="http://www.nltk.org/">NLTK</a> and processes like lemmatization and stemming. We will be using lemmatization for our work.</p>\n<h3 id="data-selection-decision">Data Selection Decision</h3>\n<p><em>Wednesday, September 20th</em></p>\n<p>After looking through different sources of data this week and talking with my research mentor, it has been decided that using <a href="https://www.ncbi.nlm.nih.gov/pmc/">PubMed Central</a> as the sole data source should give enough documents to have good results. Furthermore, the data source contains parsed documents in XML format which allows for minimal use of additional parsing techniques from PDF format.</p>\n<h3 id="read-through-papers">Read through papers</h3>\n<p><em>Wednesday, September 13th</em></p>\n<p>After being introduced to subjects by reading research papers, I have a decent understanding of the process that is taken to get from the step of text extraction and input to hypothesis generation (with lack of understanding of some specifics). I will be speaking with Justin some time this week in order to work out some questions I have about the process, but overall seems like a very cool process.</p>\n<p>I am now starting to look through different sources of data. The main and quickest source to find is <a href="https://www.ncbi.nlm.nih.gov/pmc/">PubMed Central</a>, but—given that this should have on the order of a couple of million of articles—this could be enough if no other viable source of data is found.</p>\n<p>I have also been looking into different programs for parsing text from PDF format. ### Beginning of Research</p>\n<p><em>Wednesday, September 6th</em></p>\n<p>Met with Dr. Safro and Dr. Herzog, and Justin Sybrandt from the ACS Lab. Introduced to and talked with Justin about his research and where my additional work will fit in.</p>\n<p>What I will be doing is working on reapplying Topic Modeling and Hypothesis Generation with abstracts and also full-text research papers as data input. This will require:</p>\n<ol type="1">\n<li>Reading through paper on <a href="https://arxiv.org/abs/1702.06176">MOLIERE: Automatic Biomedical Hypothesis Generation System</a> as well as others in order to become introduced to research at large within this particular area.\n<ul>\n<li>Others Include\n<ul>\n<li><a href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-11-492">The structural and content aspects of abstracts versus bodies of full text journal articles are different</a></li>\n<li><a href="https://www.biorxiv.org/content/biorxiv/early/2017/07/11/162099.full.pdf">Text mining of 15 million full-text scientific articles</a></li>\n</ul></li>\n</ul></li>\n<li>Finding of papers in large enough size so that reliable results and valid comparisons can be made.</li>\n<li>Parsing out text from papers and removing things like, equations, tables, image links and references.</li>\n</ol>\n</body>\n</html>\n'}} />)} />
            <Route path='/projects/multi-robot_environment' component={() => (<div dangerouslySetInnerHTML={{ __html: '<!DOCTYPE html>\n<html>\n<head>\n  <meta charset="utf-8">\n  <meta name="generator" content="pandoc">\n  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">\n  <title></title>\n  <style type="text/css">code{white-space: pre;}</style>\n  <!--[if lt IE 9]>\n    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>\n  <![endif]-->\n</head>\n<body>\n<h1 id="research-science-internship-dimacs-reu-at-rutgers-university">Research Science Internship @ DIMACS REU at Rutgers University</h1>\n<p>Feel free to download and look at my <a href="../Resources/finalResearchPaper.pdf">research paper</a>.</p>\n<h2 id="discrete-representation-of-multi-agent-environment">Discrete Representation of multi-agent environment</h2>\n<img src="../Resources/DiscreteSmaller.gif" />\n<center>\n<em>Discrete Multi-Robot Environment Simulation</em>\n</center>\n<h3 id="diagram-explanation">Diagram explanation</h3>\n<p>This is a simulation of a multi-robot environment in which basic path planning is implemented. It is done in a discrete case to allow for easier path planning. The grid size and hexagonal shape is chosen so that a disc with radius <strong>r</strong> will not collide with another disc of radius <strong>r</strong>.</p>\n<p>Pseudo code</p>\n<ol type="1">\n<li>A Starting and Ending point on the graph are randomly chosen for each disc.</li>\n<li>A breadth first search is done from each discs goal node, that is saved on the node data structure</li>\n<li>On each discrete transition, every node tries to move closer to its goal node, while trying not to run into another disc, using info on the nodes</li>\n<li>Random priorities are decided among the discs and the next step is computed.</li>\n<li>Repeat step 4 until all nodes are at their respective goal states.</li>\n</ol>\n<h2 id="work-summary-and-blog">Work Summary And Blog</h2>\n<p>For this project, I was responsible for generating simulations for multi-robot environments. These simulations would be used in order to determine if there are any sharp-transitions in the behavior of multi-robot systems when the constraints of the system changed, specifically the locality and density of robots in the system.</p>\n<p>Overall, I looked at three different scenarios under these constraints: Continuous, Discrete, and Discrete with Collision avoidance. Often times problems that have to do with a continuous amount of possibilities is simplified to a discrete case to stop it from being able to be solved in a reasonable amount of time, so that was why continuous with path planning was not done.</p>\n<p>In the end, it was determined that there were no sharp transitions in the specific environments and scenarios that I explored. Further research can be done to see what types of properties would lead to such transitions, but as of now my simulations and path determination algorithm do not create sharp transitions.</p>\n<figure>\n<img src="../Resources/ContinuousSmaller.gif" />\n</figure>\n<center>\n<em>Continuous Case</em>\n</center>\n<h2 id="final-week-and-a-research-paper">Final week and a Research Paper</h2>\n<p><em>Week 9</em></p>\n<p>This week, I am continuing to write my final research paper based upon the results that were found. I would like to thank DIMACS and NSF for the funding that I received and also for the great Summer of Research that I had; it was an unforgettable experience.</p>\n<h2 id="final-data-collection">Final data collection</h2>\n<p><em>Week 8</em></p>\n<p>This week, I am collecting more data. Also, working on writing the final research paper and the summary of my experience in the program.</p>\n<h2 id="lack-of-a-sharp-transition">Lack of a sharp transition</h2>\n<p><em>Week 7</em></p>\n<p>This week, I worked more with locality and discovered moreso how it affects the number of collisions overall. There were no unexpected results however and there was also no perceiveable sharp transition. So far it seems like I would have to increase the sophistication for the collision avoidance to get a sharp transition.</p>\n<h2 id="locality-and-disc-radius">Locality and disc radius</h2>\n<p><em>Week 6</em></p>\n<p>This week, I worked more on data collection and exploring the solution. I implemented the locality based on edge distance generation for starting and ending nodes for the discs. I also collected data based on the locality and total discs in the environment as without the locality the results were predictable and there was no sharp transition.</p>\n<h2 id="more-data-and-more-deterministic">More Data and more deterministic</h2>\n<p><em>Week 5</em></p>\n<p>I gathered basic data on the collision behavior, but the more random method of determining solutions to the problem caused a few problems with the speed of solving and thus the speed of collecting data. I worked this week on making the process deterministic. I also collected some more data.</p>\n<h2 id="trying-out-the-gpu">Trying out the GPU</h2>\n<p><em>Week 4</em></p>\n<p>This week, I generated graphs based on how the locality of travel and density of the discs area in a unit square affect the number of collisions. Using plot.ly along with python, I was able to run simulations on the number of pairwise collisions. In the system there appeared to be no sharp transitions.</p>\n<h2 id="i-now-have-graphs">I now have graphs</h2>\n<p><em>Week 3</em></p>\n<p>This week, I generated graphs based on how the locality of travel and density of the discs area in a unit square affect the number of collisions. Using plot.ly along with python, I was able to run simulations on the number of pairwise collisions. In the system there appeared to be no sharp transitions.</p>\n<h2 id="onwards">Onwards!</h2>\n<p><em>Week 2</em></p>\n<p>This week, I ran simulations to determine how the properties of the system affected the interaction between the discs. The results of this step are so far inconclusive since no sharp transition was found yet. I set up python and pygame within visual studios and then started to work in/learn python to create a grid for the discs to be on. This will be useful later for creating models with pygame.</p>\n<h2 id="start-of-the-program">Start of the Program</h2>\n<p><em>Week 1</em></p>\n<p>This week I worked on putting together a presentation as well as making some headway into gathering data on how the number of discs, the radius, and maximum distance from starting point to ending point affect the number of pairwise collisions that occur. I layed out a plan for what I am going to be researching as well as getting my workspace organized and set up for working. I met with my mentor, Jingjin Yu, and we layed out more-so what I will be starting with.</p>\n</body>\n</html>\n'}} />)} />
            <Route path='/projects/pubsub_architecture_analysis' component={() => (<div dangerouslySetInnerHTML={{ __html: '<!DOCTYPE html>\n<html>\n<head>\n  <meta charset="utf-8">\n  <meta name="generator" content="pandoc">\n  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">\n  <title></title>\n  <style type="text/css">code{white-space: pre;}</style>\n  <!--[if lt IE 9]>\n    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>\n  <![endif]-->\n</head>\n<body>\n<h1 id="research-with-human-centered-cloud-robotics-group-clemson">Research with Human-Centered Cloud Robotics group @ Clemson</h1>\n<h2 id="simulation-of-networks-effect-on-real-time-environemnt">Simulation of network\'s effect on real-time environemnt</h2>\n<p><img src="../Resources/pubSubRep.png" /> ### Tools for Analysis</p>\n<p><em>Tuesday, November 15th, 2016</em></p>\n<p>I mentioned previously that I will be looking at how well different publish and subscribe architectures perform in real time environments. The specific environment that I am looking at is one in which a controller will be controlling, through a broker and a plant, a tiltable plate with a ball on it. this allows for us to analyze the behavior over time and how well it actually performs the task, without any predictive behaviors. The following figures are the basis of my visual analysis, since the ball-plate model is a simulation in the first place. There are really just two things that I am looking at right now, how far the ball is from its desired location in the x and y direction and a histogram of how long the sending of data actually takes.</p>\n<figure>\n<img src="../Resources/figure_1.png" />\n</figure>\n<center>\n<em>Distribution of Time Taken</em>\n</center>\n<p><img src="../Resources/figure_2.png" /> <img src="../Resources/figure_3.png" /></p>\n<center>\n<em>How Far from Path</em>\n</center>\n<p>These diagrams will generally help with basic analysis, but numerical analysis and comparisons will be done as well.</p>\n<h3 id="mininet">Mininet</h3>\n<p><em>Wednesday, November 2nd, 2016</em></p>\n<p>Over the past few weeks, I have been working on creating network topologies with Mininet[1], then testing the performance of MQTT QoS 2 in order to run a ball-plate simulation. A ball-plate simulation is one in which we literally simulate three entities: a ball being balanced on plate, an observer (plant), and a controller. The observer desires the ball to be in a certain location on the plate and also in constant motion. The controller just listens the position that the observer wants and tries to make it happen if it can.</p>\n<p>Now the specific network topology only consists of 3 hosts and their respective switches; the hosts are called Broker, Plant, and Controller. The topology is shown in the diagram below.</p>\n<figure>\n<img src="../Resources/diagram1.png" />\n</figure>\n<center>\n<em>Network Topology</em>\n</center>\n<p>Although a basic topology, it will be perfect for simple analysis of the effect of latency and time delay between transmissions for the data between the host, broker, and plant. Originally there were some problems with creating the network topology, because I was not running a controller on my machine.</p>\n<h3 id="simulating-network-topology-with-mininet">Simulating Network Topology with Mininet</h3>\n<p><em>Wednesday, October 12th, 2016</em></p>\n<p>I have also been looking into some of the features of Mininet, which is an application (or an extension of Linux) that allows you to construct a network and create connections between virtual hosts and servers within a network. Actually, the specific structure that I am wanting to create is one in which there is a publisher, a subscriber, and a broker or a controller, plant, and broker. After I achieve that, I will be adding more and more subscribers and/or publishers and then study the amount of traffic that goes through the network and how real time tasks are affected by the changes on the load within the server and Network. I will then be adding Kafka and MQTT into the mix and comparing the two. Not only comparing generic MQTT and Kafka, but the three levels of MQTT (explained below) and Kafka. Right now, I am currently working more and more with Mininet in order to learn how to accomplish these tasks. [3]</p>\n<h3 id="learning-more-about-kafka-and-mqtt">Learning more about Kafka and MQTT</h3>\n<p><em>Tuesday, October 11th, 2016</em></p>\n<p>Over the past week I have been looking more into Kafka and MQTT in order to get a better understanding of their purposes and drawbacks. I spoke with my mentor, Dr. Remy, and looked at some of the drawbacks and benefits of these publish and subscribe architecture with respect to the amount of overhead that is associated with each. Overall, it seems that Kafka processes less information in the server and then sends more data in order to achieve the same task on real-time process, whereas MQTT seems to process more within the servers and then send less data overall. Hence the architecture, MQTT, living up to its name of mosquito. MQTT has three levels of service, unlike Kafka which is always guaranteed at least once delivery; MQTT can be at most once but maybe not at all (QoS 0), at least once (QoS 1), and at most once (QoS 2) whereas Kafka is usually only guaranteeing at least once delivery. This creates the possibility for a lot of data being sent over the network, but is also quicker than at most once delivery, because of the lack of a 3 way handshake. [1, 2] I am curious about what the size of the Kafka messages were that were stored on the server, because if the link to the next message is just as long as the message, then we will definitely get a much larger amount of data sent. Overall, it seems that the larger the message being sent in Kafka, the more the overhead shrinks in comparison; in other words, the header does not need to change size when the message size changes by that much so the theoretically optimal thing to do is to have a message of infinite length and then the overhead would be zero. (not really feasible but makes mathematical sense).</p>\n<h3 id="learning-more-about-tcp">Learning More about TCP</h3>\n<p><em>Monday, October 10th, 2016</em></p>\n<p>Last week I worked on our Networking class project for TCP. I started out with just a basic skeleton file with the basic necessities of establishing a TCP connection between an echo server and an echo client. I made a basic overarching protocol where header information was sent first with the amount of bytes that were going to be sent, and the number of messages that would be sent. With this, I could declare if there were going to be more messages and how many more would be sent. This way there would be no way for the server to close the connection before the client was done communicating with the server. The server then did the simple operation of just inverting the characters in the array; it just switched the case of the letters. This was a toy project just to learn how to use TCP and how to guarantee that the connection does not close before the message is done sending.</p>\n<h3 id="dweepy-and-dweet.io-analysis">Dweepy and Dweet.io Analysis</h3>\n<p><em>Tuesday, September 6th, 2016</em></p>\n<p>This past week I worked on creating software for in depth analysis of the sending and receiving procedure for dweet.io! I started with creating software to time how long it takes for a message to be published and a message to be received when using dweepy. Then, using tcpdump, I analyzed how many messages were sent and received. Using pexpect, a python library for scripting, I ran tcpdump as a child process and used it to analyze things like, how many sends, time between sends and receives, the number of sends and receives, the number of bytes sent, and finally the average ordering of the send receive procedures as well as the percentage of n messages being sent over the trials.</p>\n<p>After collecting the data, we can analyze the amount of overhead that exists for sending an integer (4 bytes) and a string of length 10 (10 bytes) for a total of 14 bytes. For both publishing and subscribing, we receive 5,500 bytes and send 1,500 bytes. Also, I looked at the time it takes to get from the dweet.io to the first requests, which doesn\'t seem to be that long on average. Most of the time was spent during the send and receives between the server and client; there were 16 send/receives in totality.</p>\n<p>One problem with my method was that I was waiting 1 second with pexpect in order to see if anymore messages would be sent and received, but this gave the possibility for outliers to be thrown into the mix which resulted in some strange time problems. (Negative time results) Overall, the quality of the results so far is best when looking at the average ordering and the number of bytes sent. In order to improve, I will rerun the trials with a longer delay in between functions in order to catch the outliers that take a long time to send a response.</p>\n<p>It is interesting that so much data is actually sent to publish such a small message, but further research must be done to see what can be removed from the process. I will start working with pycurl and implement my own method for communicating with the server and see how that affects the overall times.</p>\n</body>\n</html>\n'}} />)} />
            <Route path='/projects/building_websites' component={() => (<div dangerouslySetInnerHTML={{ __html: '<!DOCTYPE html>\n<html>\n<head>\n  <meta charset="utf-8">\n  <meta name="generator" content="pandoc">\n  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">\n  <title></title>\n  <style type="text/css">code{white-space: pre;}</style>\n  <!--[if lt IE 9]>\n    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>\n  <![endif]-->\n</head>\n<body>\n<h1 id="my-website">My Website</h1>\n<p><strong>Built with react, autogenerated from markdown documents and directory structure, with bootstrap layered on top of it</strong></p>\n<p>This website is compiled and then statically served, which makes it good for use on any web server. I know now that there is <a href="https://gohugo.io/">Hugo</a>, but I enjoyed the process of putting all this together.</p>\n<p>All the html that is presented before you, is generated directly from the directory layout of files. Directories are turned into drop-down menus. (This does however become difficult when it is a multi-level drop-down, which I plan to have if time allows)</p>\n<p>Code is on <a href="https://github.com/acarrab/acarrab.github.io">Github</a></p>\n<h2 id="there-was-also-my-other-website">There was also my other website</h2>\n<p>This <a href="/OldWebsite">older website</a> was built for fun and from a low level. I think it is really fun to travel around though! I made <strong><em>interesting</em></strong> design choices though...</p>\n<p>I would have kept using this, if it wasn\'t for the need to have good mobile support.</p>\n<p><a href="/OldWebsite"><img src="../Resources/OldWebsiteScreenCap.png" /></a></p>\n</body>\n</html>\n'}} />)} />
            <Route path='/projects/deep_learning' component={() => (<div dangerouslySetInnerHTML={{ __html: '<!DOCTYPE html>\n<html>\n<head>\n  <meta charset="utf-8">\n  <meta name="generator" content="pandoc">\n  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">\n  <title></title>\n  <style type="text/css">code{white-space: pre;}</style>\n  <!--[if lt IE 9]>\n    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>\n  <![endif]-->\n</head>\n<body>\n<h1 id="deep-learning-projects">Deep learning Projects</h1>\n<p>Right now, I am working on a handful of projects that pertain to deep learning as well as making my way through <a href="http://www.deeplearningbook.org/">this book</a>. This started from long-running interest in machine learning. After taking some of the math classes that pertain to deep learning (Linear Algebra, Discrete Mathematics, Theory of Probability, Differential Equations, and Multi-variable Calculus), I want to learn deep learning in <em>depth</em>.</p>\n<h2 id="senior-capstone-project">Senior Capstone Project</h2>\n<p>Currently working on a senior capstone project in which we have been entrusted 2 <strong>NVIDIA Tesla P100 GPUs</strong>. These are GPUs that are specifically made to tackle some of the biggest problems in Machine Learning. Our group has been working on creating the proper infrastructure that will be able to efficiently and without additional bottlenecks.</p>\n<p>I specifically have been researching different hardware requirements for the GPUs, working on finding some good examples that showcase their powerful performance. So far, our group--through one of our clemson faculty members--is ordering components, like a server rack and other necessities.</p>\n<h3 id="goals-for-the-project">Goals for the project</h3>\n<p>We are working on a handful of goals, but here are the basics</p>\n<ol type="1">\n<li>Setting up Tesla P100 Architecture</li>\n<li>Take online classes in order to learn more about deep-learning frameworks</li>\n<li>Implement some projects within the system after setup</li>\n<li>Benchmark the performance of the machine against other machines in the department</li>\n<li>Create tutorial tailored for students in order to give them familiarity with the system</li>\n<li>Create fun example in order to showcase one of many cool projects within the Computer Science Department at Clemson</li>\n</ol>\n<h2 id="technical-writing">Technical Writing</h2>\n<p>(Yes even within Technical Writing)</p>\n<p>Our class is making a showcase and giving a presentation of the new technologies of that are available through new IoT/Big Data. Beyond that, a large aspect of the project is our groups teaming up with groups at University of Braunschweig in Germany.</p>\n<p>Our group, on the other hand, is taking this opportunity to showcase real-time Object-Recognition and Classification using some neat libraries. (However, we have not yet decided between Tensor Flow, DIGITS, or Watson\'s object recognition software)</p>\n<h2 id="personal-goals">Personal Goals</h2>\n<p>I am working on learning more of the math in order to be able to create tailored machine learning models in order to tackle real-world problems. However, I do not want to use something so cool and not really know what is going on underneath the hood.</p>\n<p>My routines for learning consist of</p>\n<ul>\n<li>Reading through <a href="http://www.deeplearningbook.org/">this textbook</a></li>\n<li>Watching <a href="https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A">Siraj</a></li>\n<li>Trying to get into a course on <a href="https://www.udacity.com/">Udacity</a> for deep learning</li>\n</ul>\n</body>\n</html>\n'}} />)} />
            <Route path='/school_work/2015_Spring_102_RayTracer' component={() => (<div dangerouslySetInnerHTML={{ __html: '<!DOCTYPE html>\n<html>\n<head>\n  <meta charset="utf-8">\n  <meta name="generator" content="pandoc">\n  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">\n  <title></title>\n  <style type="text/css">code{white-space: pre;}</style>\n  <!--[if lt IE 9]>\n    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>\n  <![endif]-->\n</head>\n<body>\n<h1 id="ray-tracer">Ray Tracer</h1>\n<p><em>Spring 2015</em></p>\n<p>Wrote ray-tracer code in order to render a spaceship. The core aspects of the class were to be introduced to c++, but using additional knowledge gained through classes like Multi-Variable calculus, I was able to create unique 3d structures. Also implemented texture mapping to a sphere, which is showcased in the photo.</p>\n<h3 id="result">Result</h3>\n<figure>\n<img src="../Resources/SpaceRaytracer.jpg" />\n</figure>\n</body>\n</html>\n'}} />)} />
            <Route path='/school_work/2017_Spring_Computer_Graphics' component={() => (<div dangerouslySetInnerHTML={{ __html: '<!DOCTYPE html>\n<html>\n<head>\n  <meta charset="utf-8">\n  <meta name="generator" content="pandoc">\n  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">\n  <title></title>\n  <style type="text/css">code{white-space: pre;}</style>\n  <!--[if lt IE 9]>\n    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>\n  <![endif]-->\n</head>\n<body>\n<h1 id="computer-graphics-projects">Computer Graphics Projects</h1>\n<h2 id="teapot-with-rendering-equation-approximation">Teapot with Rendering Equation approximation</h2>\n<p><em>Late Spring 2016</em></p>\n<p>This project was done in order to approximate the light scattering effects that occur within real-world environments.</p>\n<p>Using <a href="http://graphics.stanford.edu/courses/cs348b-10/lectures/renderingequation/renderingequation.pdf">Radiosity equations</a> and randomized sampling around spheres according approximate things like light scatter as well.</p>\n<h3 id="result">Result</h3>\n<figure>\n<img src="../Resources/renderingEquationTeapot.png" />\n</figure>\n<h2 id="stanford-bunny">Stanford Bunny</h2>\n<p><em>Early Spring 2016</em></p>\n<p>For this project, our group worked with the classic stanford bunny and applied some of our newly learned knowledge in computer graphics to generate a model of the bunny and make it look pretty.</p>\n<p>In hindsight, the motion blur was too much :(</p>\n<figure>\n<img src="../Resources/stanfordBunny.png" />\n</figure>\n</body>\n</html>\n'}} />)} />
            <Route path='/school_work/2017_Spring_2D_Game_Engine_Design' component={() => (<div dangerouslySetInnerHTML={{ __html: '<!DOCTYPE html>\n<html>\n<head>\n  <meta charset="utf-8">\n  <meta name="generator" content="pandoc">\n  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">\n  <title></title>\n  <style type="text/css">code{white-space: pre;}</style>\n  <!--[if lt IE 9]>\n    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>\n  <![endif]-->\n</head>\n<body>\n<h1 id="d-game-engine-design">2D Game Engine Design</h1>\n<p><em>Spring 2017</em></p>\n<p>This was a class intending on improving object oriented code quality and c++ understanding. All students worked on a semester long project with the end result of having a game being built and functional. For this task, the only tool we used other than stl was the <a href="https://www.libsdl.org/">SDL (Simple Direct Media Layer) 2.0</a> framework.</p>\n<h2 id="my-game">My game</h2>\n<p>I wanted to do something where I incorporated fun and amusing particle physics within the game. What better way to do this than to do this with a particle wizard! Using quick collision checking methods and some design patterns to improve things like speed and memory efficiency, the game is able to support the firing and handling of many particles in the system.</p>\n<video width="640" height="480" controls>\n<source src="../Resources/WizardGame.mp4" type="video/mp4">\n</source>\n</video>\n</body>\n</html>\n'}} />)} />
            <Route path='/school_work/fun_courses' component={() => (<div dangerouslySetInnerHTML={{ __html: '<!DOCTYPE html>\n<html>\n<head>\n  <meta charset="utf-8">\n  <meta name="generator" content="pandoc">\n  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">\n  <title></title>\n  <style type="text/css">code{white-space: pre;}</style>\n  <!--[if lt IE 9]>\n    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>\n  <![endif]-->\n</head>\n<body>\n<h1 id="funrecommended-clemson-courses-to-take">Fun/Recommended Clemson Courses to take</h1>\n<p>These are classes I have taken out of interest or thought would help me with my academic goals.</p>\n<h3 id="fall-2017">Fall 2017</h3>\n<ul>\n<li>Senior Capstone Project 4910</li>\n<li>Focusing on learning deep learning concepts and some infrastructure specifics</li>\n<li>Programming Systems</li>\n<li>The <em>how it\'s made</em> of programming languages</li>\n</ul>\n<h3 id="spring-2017">Spring 2017</h3>\n<ul>\n<li>Design and Analysis of Algorithms 8400</li>\n<li>Learned advanced runtime analysis and better application of known algorithms</li>\n<li>Data Science</li>\n<li>Learned high level information about statistical models</li>\n<li>Computer Graphics</li>\n<li>Fun geometry and calculus as well as well as the need to work in highly constrained environment</li>\n</ul>\n<h3 id="fall-2016">Fall 2016</h3>\n<ul>\n<li>Networks and Network Programming</li>\n<li>Worked on Honors research (with Human-Centered Cloud Robotics) while in this class</li>\n</ul>\n<h3 id="spring-2016">Spring 2016</h3>\n<ul>\n<li>Ordinary Differential Equations (Honors Version)</li>\n<li>Linear Algebra</li>\n</ul>\n<h3 id="fall-2015">Fall 2015</h3>\n<ul>\n<li>Algorithms and Data Structures</li>\n<li>Still in top 5 favorite classes</li>\n<li>Organic Chemistry</li>\n<li>Fun class that teaches students to work with abstract, feature-described, objects and try to predict their interactions.</li>\n</ul>\n<h3 id="spring-2015">Spring 2015</h3>\n<ul>\n<li>Calculus of Several Variables</li>\n<li>Comp Sci 102</li>\n<li>First project heavy computer science course</li>\n<li>This is where I made <a href="#/school_work/2015_Spring_%7C_102_RayTracer">the ray-tracer</a> and code for modeling the scene</li>\n</ul>\n</body>\n</html>\n'}} />)} />
        </Switch>
    );
}