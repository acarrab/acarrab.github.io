/* Auto generated tsx */

import React from "react";
import ReactMarkdown from "react-markdown";
import { Route, Switch } from "react-router-dom";

export class Page {
    text: string
    name: string
    route: string 
}
export class Directory {
    title: string
    pages: Array<Page>
    children: Array<Directory>
}

export var Content: Directory = {
    title: 'Content',
    pages: [
        {
            name: "Home",
            text: "# Welcome\n\n## Here are some things you can do here\n\n1. Please check out my [about page](#/about), it has some information and project history.\n1. If you are bored, you can visit my [Resume](../Resume/Resume.pdf).\n1. If you are really bored, you can look at my [GitHub](https://github.com/acarrab).\n1. If you are extremely bored, you can browse [this](https://imgur.com/r/FunnyAnimals).\n\n## Current Work\n\n### Research\n\n- [Topic Modeling and Hypothesis Generation](#/projects/topic_modeling_and_hypothesis_generation)\n  - Applying [LDA](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) topic modeling to large corpus (1.7 million documents) of medical research papers from [PubMed Central](https://www.ncbi.nlm.nih.gov/pmc/)\n\n### Personal/Academic\n\n- Learning more about different statistical machine learning processes, particularly deep-learning\n  - [more info](#/projects/deep_learning)\n\n\n## Past Work\n- [Multi-agent Environment Research (REU)](#/projects/multi-robot_environment)\n- [PubSub Architecture Analysis](#/projects/pubsub_architecture_analysis)",
            route: "/"
        },
        {
            name: "About",
            text: "# About me\n\nHello people, this page is coming to you live from a server somewhere on the interweb!\n\nAnyways, my name is Angelo and I am a Senior Computer Science Student at\nClemson. I do things and some of those things are, well, on this website.\n\nI like long walks in the park and most music. I also liked the work I have done in the past, which is below.\n\n## Honors Thesis Work\n\n*2017-2018 (present)* \n\n**[MORE INFO](#/projects/topic_modeling_and_hypothesis_generation)**\n\n\nYear long project with goal of improving existing techniques for hypothesis generation by extracting relevant information from research papers.\n\nI am working with big data (1.7 million Medical Research Articles from [PubMed Central](https://www.ncbi.nlm.nih.gov/pmc/))\n\n## Software engineering intern at [Blackbaud](https://www.blackbaud.com/)\n\n*Summer 2017*\n\n![](../Resources/blackbaud.jpg)\n\n<center><em>That is me kneeling at the bottom right</em></center>\n\nWorked on a Scrum team within an ASP.net web application, called Raisers Edge NXT, with Angular 2 on the front-end and C# and SQL on the back-end. Updated and added health pages that gave developers the tools to better analyze database health and edit database package status. Reduced the number of health pages by making a more centralized location for health data.\n\nSpecifically, wrote JavaScript with Angular 2 Components and Services. As well as JavaScript Jasmine unit tests. On the back-end wrote in C# with SpecFlow unit tests and integration tests. Wrote basic SQL as well.\n\n### Research with Human-Centered Cloud Robotics @ Clemson University\n\n*Aug. 2016 - Jan. 2017* \n\n**[MORE INFO](#/projects/pubsub_architecture_analysis)**\n\n- Analyzed pubsub architectures performance in real-time environments, or simulations of real-time environments with no prediction.\n- Simulations done in Mininet with python scripts for generating the network topology.\n- Looked specifically at MQTT and Kafka\n\n## Research at Rutgers University with [DIMACS](http://dimacs.rutgers.edu/)\n\n*Summer 2016* \n\n**[MORE INFO](#/projects/multi-robot_environment)**\n\nJoined research experience for Undergrads program at Rutgers University.\n\n1. Wrote a [research paper](../Resources/finalResearchPaper.pdf) on results.\n1. Created a simulation in python, with graphics created with pygame.\n1. Analyze traffic in a multi-robot environment with multi-agent simulations.\n1. Specifically looking for sharp-transition in behavior in discretized traffic network when compared to continuous one.\n\n#### Takeaway\n\nDetermined through self-created simulations that there were no sharp transition in traffic, based on changes in robot size and start-end locality.\n\n## Applied Algorithms Lab @ Clemson University \n\n*Jan 2016 - June 2016*\n\nImplemented DTW Search with modifications to improve result quality with noisy signal.\nFor EEG database to add search functionality for Neurologists at MUSC research hospital.\n\n## Virtual Environment Group @ Clemson University\n\n*Sep. 2014 - Aug. 2015*\n\nResearch Assistant C Designed computer vision software in C, OpenGL,\nand GLUT.\nWorked on software for calibrating camera image distortion through analysis of an image of a checkered board.\nCreated edge detection processing.",
            route: "/about"
        }
    ],
    children: [
        {
            title: 'Projects',
            pages: [
                {
                    name: "Topic Modeling And Hypothesis Generation",
                    text: "# Honors Research Project for Senior Year\n\nThis project is a school year long project in which I am working along side grad students in the ACS Lab (Algorithms and Computational Sciences Lab). I am adding to the work being done by [Justin Sybrandt](http://sybrandt.com/) by determining if we can increase the accuracy of Moliere, a hypothesis generation method.\n\nWe are seeing if it helps to extract additional information from the full-texts and, if so, how to best do so.\n\n## Progress\n\n*Last updated: Tuesday, October 24th 2017*\n\nAll code is parallelized and ran on the [Clemson Palmetto Cluster](https://www.palmetto.clemson.edu/palmetto/userguide_palmetto_overview.html) in order to complete tasks in reasonable amount of time.\nUsing [mpi4py](http://mpi4py.readthedocs.io/en/stable/) in order to run text extraction on the million documents in parallel on the Clemson Palmetto Cluster\n\nStarted by downloading 1.7 million documents over ftp from [PubMed Central](https://www.ncbi.nlm.nih.gov/pmc/)\n\n1. Wrote code to parse xml documents tree structure with hierarchical selection by walking through xml tree and grabbing what I need (example under **hierarchical selection example** section)\n    - Down to 1.4 million documents because: some did not have abstracts, were not research papers, or were not in proper xml format.\n1. Wrote and ran code to clean text of unicode expressions and in text equations using regular expressions.\n1. Used [NLTK](http://www.nltk.org/) in order to lemmatize text in order to be passed into the next part of the pipeline, the creation of the n_grams.\n1. Ran Abstracts through topic modeling stage of Moliere Pipeline\n\n### N-gram creation\n\n*Monday, October 30th*\n\nLearned how create n-grams through ToPMine with help of Justin, then ran the process on Abstracts document subset.\n\nCurrently only on Abstract documents and will analyze soon analyze the validity of the results that were found before running the long (maybe week long) n-gram creation on the rest of the documents.\n\n### Removal of files that are too new from the dataset\n\n*Tuesday, October 24th*\n\nWent back through pipeline that has been created so far and added a small amount of code to determine if the publish date of the article was this year as we will be removing those from the data set in order to see if prediction is valid for this year. No need for predicting things we really don't know yet, because there is no basis for comparison.\n\nAt the end of this process, it has been determined that there are now only 1.3 million articles left in totality. There are a suspicious amount of articles that are lost to being too new and I will be looking at that later, but for now am just working on lemmatizing the text using different methods.\n\nHere is the breakdown of documents (that are currently stored in separate files according to the number of processes):\n\n```bash\n[acarrab@login001 lemmatizedText]$ wc -l Abstracts_* | grep total\n   1328035 total\n```\n\nHere is the breakdown of how the documents were lost.\n\n```bash\n[acarrab@login001 acarrab]$ wc -l *.log\n  151427 failedDuringParsingOrNoAbstract.log\n   53872 hasAbstractAndFailed.log\n  164944 tooNewButHadAbstract.log\n  370243 total\n```\n\nNOTE: `tooNewButHadAbstract.log` is not necessarily accurate since it would put 3000 as year if the parsing could not find the date. \n\nI have looked at lemmatization through the means of using specialist NLP tool in order to use those to lemmatize the text, but to no avail. (A lot more learning than initially thought was needed is needed in order to use their tools).\n\nIn order to get some results, used the NLTK's lemmatizer in order lemmatize the text from the research papers. The papers are now ready for the next phase, the building of the n-grams.\n\nAfter speaking with Justin, he has seen that the benefits of using the Specialist NLP Tools is actually too slow for our data-set size in the end anyways and NLTK lemmatization does a good job and gives promising and significant results within the research that he has done.\n\n### Increasing valid parsing rate\n\n*Monday, October 16th*\n\nAfter running the jobs in order to parse the documents, added basic statistics capturing that are added to files while running batch job on the almost 1.8 million documents. At the end of the run, there are counts for different types of errors\n\n- Failure to parse xml errors\n\n- Had abstract keyword within file, but still failed\n\n- Failed for other reasons\n\nGiven this information, I can now look at the files that failed yet had the abstract keyword and determine how to reduce the number of files that are failed to be parsed.\n\n### Quality analysis of xml parsing\n\n*Wednesday, October 11th*\n\nWent through a random subset of the documents and looked at results from parsing in order to determine whether the parsing was doing what it should be doing. After making some modifications to the code written in the previous week, there is a good likelihood of valid parsing.\n\nAlso inserted failure cases for parse if things like the abstracts were missing.\n\nRan parsing parallelized on palmetto cluster and looked determined results so far. More validation should be done in order to make sure that most of the data are being used.\n\nFiles are saved in only abstract format and also full-text and abstract format in order to create distinct sets of documents that can give good comparison.\n\n### Heirarchical parsing of xml documents\n\n*Wednesday, October 4th*\n\nWrote code to parse out unicode characters as well as select out abstracts and other relevant text sections from the millions of documents.\n\nCreated method for parsing out xml documents tags that are needed in with hierarchical selection.\n\n#### Example and Explanation\n\nWrote code that extracts the following information by populating a data object with the keywords under the `\"GET\"` keyword under selection through chains of nested objects. For example, if an article title is found in the xml tree under `<article-meta><title-group><article-title>` tags, then `data[\"Title\"]` will be set to an array containing the found data.\n\nThis is a kind of cool way to extract the required information from the xml tree structure. Kind of inspired by [graphql](http://graphql.org/) selection statements in order to populate arrays of data.\n\n```python\nchains = [\n    {\n        \"article-meta\": {\n            \"title-group\": {\n                \"article-title\": {\n                    \"GET\": \"Title\"\n                }\n            }\n        },\n        \"kwd-group\": {\n            \"kwd\": {\n                \"GET\": \"Keywords\"\n            }\n        },\n        \"contrib-group\": {\n            \"contrib\": {\n                \"GETALL\": \"Contributors\"\n            }\n        },\n```\n\n### Downloading Documents\n\n*Thursday, September 28th*\n\nDownloaded the documents over ftp from [PubMed Central](https://www.ncbi.nlm.nih.gov/pmc/) and moved them onto the Palmetto Cluster.\n\nBecame familiar with some of [NLTK](http://www.nltk.org/) and processes like lemmatization and stemming. We will be using lemmatization for our work.\n\n### Data Selection Decision\n\n*Wednesday, September 20th*\n\nAfter looking through different sources of data this week and talking with my research mentor, it has been decided that using [PubMed Central](https://www.ncbi.nlm.nih.gov/pmc/) as the sole data source should give enough documents to have good results. Furthermore, the data source contains parsed documents in XML format which allows for minimal use of additional parsing techniques from PDF format.\n\n### Read through papers\n\n*Wednesday, September 13th*\n\nAfter being introduced to subjects by reading research papers, I have a decent understanding of the process that is taken to get from the step of text extraction and input to hypothesis generation (with lack of understanding of some specifics). I will be speaking with Justin some time this week in order to work out some questions I have about the process, but overall seems like a very cool process.\n\nI am now starting to look through different sources of data. The main and quickest source to find is [PubMed Central](https://www.ncbi.nlm.nih.gov/pmc/), but—given that this should have on the order of a couple of million of articles—this could be enough if no other viable source of data is found.\n\nI have also been looking into different programs for parsing text from PDF format.\n### Beginning of Research\n\n*Wednesday, September 6th*\n\nMet with Dr. Safro and Dr. Herzog, and Justin Sybrandt from the ACS Lab. Introduced to and talked with Justin about his research and where my additional work will fit in. \n\nWhat I will be doing is working on reapplying Topic Modeling and Hypothesis Generation with abstracts and also full-text research papers as data input. This will require:\n\n1. Reading through paper on [MOLIERE: Automatic Biomedical Hypothesis Generation System](https://arxiv.org/abs/1702.06176) as well as others in order to become introduced to research at large within this particular area.\n    - Others Include\n        - [The structural and content aspects of abstracts versus bodies of full text journal articles are different](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-11-492)\n        - [Text mining of 15 million full-text scientific articles](https://www.biorxiv.org/content/biorxiv/early/2017/07/11/162099.full.pdf)\n\n1. Finding of papers in large enough size so that reliable results and valid comparisons can be made. \n1. Parsing out text from papers and removing things like, equations, tables, image links and references.",
                    route: "/projects/topic_modeling_and_hypothesis_generation"
                },
                {
                    name: "Multi-Robot Environment",
                    text: "# Research Science Internship @ DIMACS REU at Rutgers University\n\nFeel free to download and look at my [research paper](../Resources/finalResearchPaper.pdf).\n\n\n## Discrete Representation of multi-agent environment\n\n![](../Resources/DiscreteSmaller.gif)\n<center><em>Discrete Multi-Robot Environment Simulation</em></center>\n\n### Diagram explanation\n\nThis is a simulation of a multi-robot environment in which basic path\nplanning is implemented. It is done in a discrete case to allow for easier\npath planning. The grid size and hexagonal shape is chosen so that a disc\nwith radius **r** will not collide with another disc of radius **r**.\n\nPseudo code\n\n1. A Starting and Ending point on the graph are randomly chosen for each disc.\n1. A breadth first search is done from each discs goal node, that is saved on the node data structure\n1. On each discrete transition, every node tries to move closer to its goal node, while trying not to run into another disc, using info on the nodes\n1. Random priorities are decided among the discs and the next step is computed.\n1. Repeat step 4 until all nodes are at their respective goal states.\n\n## Work Summary And Blog\n\nFor this project, I was responsible for generating simulations for\nmulti-robot environments. These simulations would be used in order to\ndetermine if there are any sharp-transitions in the behavior of\nmulti-robot systems when the constraints of the system changed,\nspecifically the locality and density of robots in the system.\n\nOverall, I looked at three different scenarios under these constraints:\nContinuous, Discrete, and Discrete with Collision avoidance. Often times\nproblems that have to do with a continuous amount of possibilities is\nsimplified to a discrete case to stop it from being able to be solved in\na reasonable amount of time, so that was why continuous with path\nplanning was not done.\n\nIn the end, it was determined that there were no sharp transitions in\nthe specific environments and scenarios that I explored. Further\nresearch can be done to see what types of properties would lead to such\ntransitions, but as of now my simulations and path determination\nalgorithm do not create sharp transitions.\n\n![](../Resources/ContinuousSmaller.gif)\n\n<center><em>Continuous Case</em></center>\n\n## Final week and a Research Paper\n\n*Week 9*\n\nThis week, I am continuing to write my final research paper based upon\nthe results that were found. I would like to thank DIMACS and NSF for\nthe funding that I received and also for the great Summer of Research\nthat I had; it was an unforgettable experience.\n\n## Final data collection\n\n*Week 8*\n\nThis week, I am collecting more data. Also, working on writing the final\nresearch paper and the summary of my experience in the program.\n\n\n## Lack of a sharp transition\n\n*Week 7*\n\nThis week, I worked more with locality and discovered moreso how it\naffects the number of collisions overall. There were no unexpected\nresults however and there was also no perceiveable sharp transition. So\nfar it seems like I would have to increase the sophistication for the\ncollision avoidance to get a sharp transition.\n\n##  Locality and disc radius\n\n*Week 6*\n\nThis week, I worked more on data collection and exploring the solution.\nI implemented the locality based on edge distance generation for\nstarting and ending nodes for the discs. I also collected data based on\nthe locality and total discs in the environment as without the locality\nthe results were predictable and there was no sharp transition.\n\n## More Data and more deterministic\n\n*Week 5*\n\nI gathered basic data on the collision behavior, but the more random\nmethod of determining solutions to the problem caused a few problems\nwith the speed of solving and thus the speed of collecting data. I\nworked this week on making the process deterministic. I also collected\nsome more data.\n\n## Trying out the GPU\n\n*Week 4*\n\nThis week, I generated graphs based on how the locality of travel and\ndensity of the discs area in a unit square affect the number of\ncollisions. Using plot.ly along with python, I was able to run\nsimulations on the number of pairwise collisions. In the system there\nappeared to be no sharp transitions.\n\n\n## I now have graphs\n\n*Week 3*\n\n This week, I generated graphs based on how the locality of travel and\ndensity of the discs area in a unit square affect the number of\ncollisions. Using plot.ly along with python, I was able to run\nsimulations on the number of pairwise collisions. In the system there\nappeared to be no sharp transitions.\n\n## Onwards!\n\n*Week 2*\n\n This week, I ran simulations to determine how the properties of the\nsystem affected the interaction between the discs. The results of this\nstep are so far inconclusive since no sharp transition was found yet. I\nset up python and pygame within visual studios and then started to work\nin/learn python to create a grid for the discs to be on. This will be\nuseful later for creating models with pygame.\n\n\n## Start of the Program\n\n*Week 1*\n\n This week I worked on putting together a presentation as well as making\nsome headway into gathering data on how the number of discs, the radius,\nand maximum distance from starting point to ending point affect the\nnumber of pairwise collisions that occur. I layed out a plan for what I\nam going to be researching as well as getting my workspace organized and\nset up for working. I met with my mentor, Jingjin Yu, and we layed out\nmore-so what I will be starting with.",
                    route: "/projects/multi-robot_environment"
                },
                {
                    name: "Pubsub Architecture Analysis",
                    text: "# Research with Human-Centered Cloud Robotics group @ Clemson\n## Simulation of network's effect on real-time environemnt\n![](../Resources/pubSubRep.png)\n### Tools for Analysis\n\n*Tuesday, November 15th, 2016*\n\nI mentioned previously that I will be looking at how well different publish and subscribe architectures perform in real time environments.\nThe specific environment that I am looking at is one in which a controller will be controlling, through a broker and a plant, a tiltable plate with a ball on it.\nthis allows for us to analyze the behavior over time and how well it actually performs the task, without any predictive behaviors.\nThe following figures are the basis of my visual analysis, since the ball-plate model is a simulation in the first place. There are really just\ntwo things that I am looking at right now, how far the ball is from its desired location in the x and y direction and a histogram of how long the sending of\ndata actually takes.\n\n![](../Resources/figure_1.png)\n\n<center><em>Distribution of Time Taken</em></center>\n\n![](../Resources/figure_2.png)\n![](../Resources/figure_3.png)\n\n<center><em>How Far from Path</em></center>\n\nThese diagrams will generally help with basic analysis, but numerical analysis and comparisons will\nbe done as well.\n\n### Mininet\n\n*Wednesday, November 2nd, 2016*\n\nOver the past few weeks, I have been working on creating network topologies with Mininet[1], then\ntesting the performance of MQTT QoS 2 in order to run a ball-plate simulation. A ball-plate\nsimulation is one in which we literally simulate three entities: a ball being balanced on plate, an\nobserver (plant), and a controller. The observer desires the ball to be in a certain location on the\nplate and also in constant motion. The controller just listens the position that the observer wants and\ntries to make it happen if it can.\n\nNow the specific network topology only consists of 3 hosts and their respective switches; the hosts are\ncalled Broker, Plant, and Controller. The topology is shown in the diagram below.\n\n![](../Resources/diagram1.png)\n\n<center><em>Network Topology</em></center>\n\n\nAlthough a basic topology, it will be perfect for simple analysis of the effect of latency and\ntime delay between transmissions for the data between the host, broker, and plant. Originally there were\nsome problems with creating the network topology, because I was not running a controller on my machine.\n\n### Simulating Network Topology with Mininet\n\n*Wednesday, October 12th, 2016*\n\n I have also been looking into some of the features of Mininet, which is\nan application (or an extension of Linux) that allows you to construct a\nnetwork and create connections between virtual hosts and servers within\na network. Actually, the specific structure that I am wanting to create\nis one in which there is a publisher, a subscriber, and a broker or a\ncontroller, plant, and broker. After I achieve that, I will be adding more\nand more subscribers and/or publishers and then study the amount of traffic\nthat goes through the network and how real time tasks are affected by the\nchanges on the load within the server and Network. I will then be adding\nKafka and MQTT into the mix and comparing the two. Not only comparing\ngeneric MQTT and Kafka, but the three levels of MQTT (explained below)\nand Kafka. Right now, I am currently working more and more with Mininet\nin order to learn how to accomplish these tasks. [3]\n\n### Learning more about Kafka and MQTT\n\n*Tuesday, October 11th, 2016*\n\n Over the past week I have been looking more into Kafka and MQTT in order to\nget a better understanding of their purposes and drawbacks. I spoke with my\nmentor, Dr. Remy, and looked at some of the drawbacks and benefits of these\npublish and subscribe architecture with respect to the amount of overhead\nthat is associated with each. Overall, it seems that Kafka processes less\ninformation in the server and then sends more data in order to achieve the\nsame task on real-time process, whereas MQTT seems to process more within\nthe servers and then send less data overall. Hence the architecture, MQTT,\nliving up to its name of mosquito. MQTT has three levels of service, unlike\nKafka which is always guaranteed at least once delivery; MQTT can be at most\nonce but maybe not at all (QoS 0), at least once (QoS 1), and at most once\n(QoS 2) whereas Kafka is usually only guaranteeing at least once delivery.\nThis creates the possibility for a lot of data being sent over the network,\nbut is also quicker than at most once delivery, because of the lack of a 3\nway handshake. [1, 2] I am curious about what the size of the Kafka messages\nwere that were stored on the server, because if the link to the next message\nis just as long as the message, then we will definitely get a much larger\namount of data sent. Overall, it seems that the larger the message being sent\nin Kafka, the more the overhead shrinks in comparison; in other words, the\nheader does not need to change size when the message size changes by that\nmuch so the theoretically optimal thing to do is to have a message of\ninfinite length and then the overhead would be zero. (not really feasible but\nmakes mathematical sense).\n\n### Learning More about TCP\n\n*Monday, October 10th, 2016*\n\n Last week I worked on our Networking class project for TCP. I started out\nwith just a basic skeleton file with the basic necessities of establishing\na TCP connection between an echo server and an echo client. I made a basic\noverarching protocol where header information was sent first with the\namount of bytes that were going to be sent, and the number of messages that\nwould be sent. With this, I could declare if there were going to be more\nmessages and how many more would be sent. This way there would be no way\nfor the server to close the connection before the client was done\ncommunicating with the server. The server then did the simple operation of\njust inverting the characters in the array; it just switched the case of\nthe letters. This was a toy project just to learn how to use TCP and how\nto guarantee that the connection does not close before the message is done\nsending.\n\n### Dweepy and Dweet.io Analysis\n\n*Tuesday, September 6th, 2016*\n\n This past week I worked on creating software for in depth analysis of the\nsending and receiving procedure for dweet.io! I started with creating\nsoftware to time how long it takes for a message to be published and a\nmessage to be received when using dweepy. Then, using tcpdump, I analyzed\nhow many messages were sent and received. Using pexpect, a python library\nfor scripting, I ran tcpdump as a child process and used it to analyze\nthings like, how many sends, time between sends and receives, the number\nof sends and receives, the number of bytes sent, and finally the average\nordering of the send receive procedures as well as the percentage of n\nmessages being sent over the trials.\n\n After collecting the data, we can analyze the amount of overhead that\nexists for sending an integer (4 bytes) and a string of length 10\n(10 bytes) for a total of 14 bytes. For both publishing and subscribing,\nwe receive 5,500 bytes and send 1,500 bytes. Also, I looked at the time it\ntakes to get from the dweet.io to the first requests, which doesn't seem to\nbe that long on average. Most of the time was spent during the send and\nreceives between the server and client; there were 16 send/receives in\ntotality.\n\n One problem with my method was that I was waiting 1 second with pexpect\nin order to see if anymore messages would be sent and received, but this\ngave the possibility for outliers to be thrown into the mix which\nresulted in some strange time problems. (Negative time results) Overall,\nthe quality of the results so far is best when looking at the average\nordering and the number of bytes sent. In order to improve, I will rerun\nthe trials with a longer delay in between functions in order to catch the\noutliers that take a long time to send a response.\n\n It is interesting that so much data is actually sent to publish such a\nsmall message, but further research must be done to see what can be\nremoved from the process. I will start working with pycurl and implement\nmy own method for communicating with the server and see how that affects\nthe overall times.",
                    route: "/projects/pubsub_architecture_analysis"
                },
                {
                    name: "Building Websites",
                    text: "# My Website\n\n**Built with react, autogenerated from markdown documents and directory structure, with bootstrap layered on top of it**\n\nThis website is compiled and then statically served, which makes it good for use on any web server.\nI know now that there is [Hugo](https://gohugo.io/), but I enjoyed the process of putting all this together.\n\nAll the html that is presented before you, is generated directly from the directory layout of files. Directories are turned into drop-down menus. (This does however become difficult when it is a multi-level drop-down, which I plan to have if time allows)\n\nCode is on [Github](https://github.com/acarrab/acarrab.github.io)\n\n## There was also my other website\n\nThis [older website](/OldWebsite) was built for fun and from a low level. I think it is really fun to travel around though! I made ***interesting*** design choices though...\n\nI would have kept using this, if it wasn't for the need to have good mobile support.\n\n[![](../Resources/OldWebsiteScreenCap.png)](/OldWebsite)",
                    route: "/projects/building_websites"
                },
                {
                    name: "Deep Learning",
                    text: "# Deep learning Projects\n\nRight now, I am working on a handful of projects that pertain to deep learning as well as making my way through\n[this book](http://www.deeplearningbook.org/). This started from long-running interest in machine learning.\nAfter taking some of the math classes that pertain to deep learning (Linear Algebra, Discrete Mathematics, Theory of Probability, Differential Equations, and Multi-variable Calculus), I want to learn deep learning in *depth*.\n\n## Senior Capstone Project\n\nCurrently working on a senior capstone project in which we have been entrusted 2 **NVIDIA Tesla P100 GPUs**. These are GPUs that are specifically made to tackle some of the biggest problems in Machine Learning. Our group has been working on creating the proper infrastructure that will be able to efficiently and without additional bottlenecks.\n\nI specifically have been researching different hardware requirements for the GPUs, working on finding some good examples that showcase their powerful performance. So far, our group--through one of our clemson faculty members--is ordering components, like a server rack and other necessities.\n\n### Goals for the project\n\nWe are working on a handful of goals, but here are the basics\n\n1. Setting up Tesla P100 Architecture\n1. Take online classes in order to learn more about deep-learning frameworks\n1. Implement some projects within the system after setup\n1. Benchmark the performance of the machine against other machines in the department\n1. Create tutorial tailored for students in order to give them familiarity with the system\n1. Create fun example in order to showcase one of many cool projects within the Computer Science Department at Clemson\n\n## Technical Writing\n\n(Yes even within Technical Writing)\n\nOur class is making a showcase and giving a presentation of the new technologies of that are available through new IoT/Big Data.\nBeyond that, a large aspect of the project is our groups teaming up with groups at University of Braunschweig in Germany.\n\nOur group, on the other hand, is taking this opportunity to showcase real-time Object-Recognition and Classification using some neat libraries. (However, we have not yet decided between Tensor Flow, DIGITS, or Watson's object recognition software)\n\n## Personal Goals\n\nI am working on learning more of the math in order to be able to create tailored machine learning models in order to tackle real-world problems. However, I do not want to use something so cool and not really know what is going on underneath the hood.\n\nMy routines for learning consist of\n\n- Reading through [this textbook](http://www.deeplearningbook.org/)\n- Watching [Siraj](https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A)\n- Trying to get into a course on [Udacity](https://www.udacity.com/) for deep learning\n",
                    route: "/projects/deep_learning"
                }
            ],
            children: [
            ]
        },
        {
            title: 'School Work',
            pages: [
                {
                    name: "2015 Spring | 102 Raytracer",
                    text: "# Ray Tracer\n\n*Spring 2015*\n\nWrote ray-tracer code in order to render a spaceship. The core aspects of the class were to be introduced to c++, but using additional knowledge gained through classes like Multi-Variable calculus, I was able to create unique 3d structures. Also implemented texture mapping to a sphere, which is showcased in the photo.\n\n### Result\n\n![](../Resources/SpaceRaytracer.jpg)",
                    route: "/school_work/2015_Spring_102_RayTracer"
                },
                {
                    name: "2017 Spring | Computer Graphics",
                    text: "# Computer Graphics Projects\n\n## Teapot with Rendering Equation approximation\n\n*Late Spring 2016*\n\nThis project was done in order to approximate the light scattering effects that occur within real-world environments. \n\nUsing [Radiosity equations](http://graphics.stanford.edu/courses/cs348b-10/lectures/renderingequation/renderingequation.pdf) and randomized sampling around spheres according approximate things like light scatter as well.\n\n### Result\n\n![](../Resources/renderingEquationTeapot.png)\n\n\n## Stanford Bunny\n\n*Early Spring 2016*\n\nFor this project, our group worked with the classic stanford\nbunny and applied some of our newly learned knowledge in computer\ngraphics to generate a model of the bunny and make it look pretty.\n\nIn hindsight, the motion blur was too much :(\n\n![](../Resources/stanfordBunny.png)",
                    route: "/school_work/2017_Spring_Computer_Graphics"
                },
                {
                    name: "2017 Spring | 2D Game Engine Design",
                    text: "# 2D Game Engine Design\n\n*Spring 2017*\n\nThis was a class intending on improving object oriented code quality and c++ understanding. All students worked on a semester long project with the end result of having a game being built and functional. For this task, the only tool we used other than stl was the [SDL (Simple Direct Media Layer) 2.0](https://www.libsdl.org/) framework.\n\n## My game\n\nI wanted to do something where I incorporated fun and amusing particle physics within the game. What better way to do this than to do this with a particle wizard! Using quick collision checking methods and some design patterns to improve things like speed and memory efficiency, the game is able to support the firing and handling of many particles in the system.\n\n<video width='640' height='480' controls>\n<source src='../Resources/WizardGame.mp4' type='video/mp4'>\n</source>\n</video>\n",
                    route: "/school_work/2017_Spring_2D_Game_Engine_Design"
                },
                {
                    name: "Fun Courses",
                    text: "# Fun/Recommended Clemson Courses to take\n\nThese are classes I have taken out of interest or thought would help me with my academic goals.\n\n### Fall 2017\n\n- Senior Capstone Project 4910\n  - Focusing on learning deep learning concepts and some infrastructure specifics\n- Programming Systems\n  - The *how it's made* of programming languages\n\n### Spring 2017\n\n- Design and Analysis of Algorithms 8400\n  - Learned advanced runtime analysis and better application of known algorithms\n- Data Science\n  - Learned high level information about statistical models\n- Computer Graphics\n  - Fun geometry and calculus as well as well as the need to work in highly constrained environment\n\n### Fall 2016\n\n- Networks and Network Programming\n  - Worked on Honors research (with Human-Centered Cloud Robotics) while in this class\n\n### Spring 2016\n\n- Ordinary Differential Equations (Honors Version)\n- Linear Algebra\n\n### Fall 2015\n\n- Algorithms and Data Structures\n  - Still in top 5 favorite classes\n- Organic Chemistry\n  - Fun class that teaches students to work with abstract, feature-described, objects and try to predict their interactions.\n\n### Spring 2015\n\n- Calculus of Several Variables\n- Comp Sci 102\n  - First project heavy computer science course\n  - This is where I made [the ray-tracer](#/school_work/2015_Spring_|_102_RayTracer) and code for modeling the scene",
                    route: "/school_work/fun_courses"
                }
            ],
            children: [
            ]
        }
    ]
}

export function CompiledRoutes() {
    return (
        <Switch>
            <Route exact path='/' component={() => (<ReactMarkdown source={"# Welcome\n\n## Here are some things you can do here\n\n1. Please check out my [about page](#/about), it has some information and project history.\n1. If you are bored, you can visit my [Resume](../Resume/Resume.pdf).\n1. If you are really bored, you can look at my [GitHub](https://github.com/acarrab).\n1. If you are extremely bored, you can browse [this](https://imgur.com/r/FunnyAnimals).\n\n## Current Work\n\n### Research\n\n- [Topic Modeling and Hypothesis Generation](#/projects/topic_modeling_and_hypothesis_generation)\n  - Applying [LDA](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) topic modeling to large corpus (1.7 million documents) of medical research papers from [PubMed Central](https://www.ncbi.nlm.nih.gov/pmc/)\n\n### Personal/Academic\n\n- Learning more about different statistical machine learning processes, particularly deep-learning\n  - [more info](#/projects/deep_learning)\n\n\n## Past Work\n- [Multi-agent Environment Research (REU)](#/projects/multi-robot_environment)\n- [PubSub Architecture Analysis](#/projects/pubsub_architecture_analysis)"} />)} />
            <Route path='/about' component={() => (<ReactMarkdown source={"# About me\n\nHello people, this page is coming to you live from a server somewhere on the interweb!\n\nAnyways, my name is Angelo and I am a Senior Computer Science Student at\nClemson. I do things and some of those things are, well, on this website.\n\nI like long walks in the park and most music. I also liked the work I have done in the past, which is below.\n\n## Honors Thesis Work\n\n*2017-2018 (present)* \n\n**[MORE INFO](#/projects/topic_modeling_and_hypothesis_generation)**\n\n\nYear long project with goal of improving existing techniques for hypothesis generation by extracting relevant information from research papers.\n\nI am working with big data (1.7 million Medical Research Articles from [PubMed Central](https://www.ncbi.nlm.nih.gov/pmc/))\n\n## Software engineering intern at [Blackbaud](https://www.blackbaud.com/)\n\n*Summer 2017*\n\n![](../Resources/blackbaud.jpg)\n\n<center><em>That is me kneeling at the bottom right</em></center>\n\nWorked on a Scrum team within an ASP.net web application, called Raisers Edge NXT, with Angular 2 on the front-end and C# and SQL on the back-end. Updated and added health pages that gave developers the tools to better analyze database health and edit database package status. Reduced the number of health pages by making a more centralized location for health data.\n\nSpecifically, wrote JavaScript with Angular 2 Components and Services. As well as JavaScript Jasmine unit tests. On the back-end wrote in C# with SpecFlow unit tests and integration tests. Wrote basic SQL as well.\n\n### Research with Human-Centered Cloud Robotics @ Clemson University\n\n*Aug. 2016 - Jan. 2017* \n\n**[MORE INFO](#/projects/pubsub_architecture_analysis)**\n\n- Analyzed pubsub architectures performance in real-time environments, or simulations of real-time environments with no prediction.\n- Simulations done in Mininet with python scripts for generating the network topology.\n- Looked specifically at MQTT and Kafka\n\n## Research at Rutgers University with [DIMACS](http://dimacs.rutgers.edu/)\n\n*Summer 2016* \n\n**[MORE INFO](#/projects/multi-robot_environment)**\n\nJoined research experience for Undergrads program at Rutgers University.\n\n1. Wrote a [research paper](../Resources/finalResearchPaper.pdf) on results.\n1. Created a simulation in python, with graphics created with pygame.\n1. Analyze traffic in a multi-robot environment with multi-agent simulations.\n1. Specifically looking for sharp-transition in behavior in discretized traffic network when compared to continuous one.\n\n#### Takeaway\n\nDetermined through self-created simulations that there were no sharp transition in traffic, based on changes in robot size and start-end locality.\n\n## Applied Algorithms Lab @ Clemson University \n\n*Jan 2016 - June 2016*\n\nImplemented DTW Search with modifications to improve result quality with noisy signal.\nFor EEG database to add search functionality for Neurologists at MUSC research hospital.\n\n## Virtual Environment Group @ Clemson University\n\n*Sep. 2014 - Aug. 2015*\n\nResearch Assistant C Designed computer vision software in C, OpenGL,\nand GLUT.\nWorked on software for calibrating camera image distortion through analysis of an image of a checkered board.\nCreated edge detection processing."} />)} />
            <Route path='/projects/topic_modeling_and_hypothesis_generation' component={() => (<ReactMarkdown source={"# Honors Research Project for Senior Year\n\nThis project is a school year long project in which I am working along side grad students in the ACS Lab (Algorithms and Computational Sciences Lab). I am adding to the work being done by [Justin Sybrandt](http://sybrandt.com/) by determining if we can increase the accuracy of Moliere, a hypothesis generation method.\n\nWe are seeing if it helps to extract additional information from the full-texts and, if so, how to best do so.\n\n## Progress\n\n*Last updated: Tuesday, October 24th 2017*\n\nAll code is parallelized and ran on the [Clemson Palmetto Cluster](https://www.palmetto.clemson.edu/palmetto/userguide_palmetto_overview.html) in order to complete tasks in reasonable amount of time.\nUsing [mpi4py](http://mpi4py.readthedocs.io/en/stable/) in order to run text extraction on the million documents in parallel on the Clemson Palmetto Cluster\n\nStarted by downloading 1.7 million documents over ftp from [PubMed Central](https://www.ncbi.nlm.nih.gov/pmc/)\n\n1. Wrote code to parse xml documents tree structure with hierarchical selection by walking through xml tree and grabbing what I need (example under **hierarchical selection example** section)\n    - Down to 1.4 million documents because: some did not have abstracts, were not research papers, or were not in proper xml format.\n1. Wrote and ran code to clean text of unicode expressions and in text equations using regular expressions.\n1. Used [NLTK](http://www.nltk.org/) in order to lemmatize text in order to be passed into the next part of the pipeline, the creation of the n_grams.\n1. Ran Abstracts through topic modeling stage of Moliere Pipeline\n\n### N-gram creation\n\n*Monday, October 30th*\n\nLearned how create n-grams through ToPMine with help of Justin, then ran the process on Abstracts document subset.\n\nCurrently only on Abstract documents and will analyze soon analyze the validity of the results that were found before running the long (maybe week long) n-gram creation on the rest of the documents.\n\n### Removal of files that are too new from the dataset\n\n*Tuesday, October 24th*\n\nWent back through pipeline that has been created so far and added a small amount of code to determine if the publish date of the article was this year as we will be removing those from the data set in order to see if prediction is valid for this year. No need for predicting things we really don\'t know yet, because there is no basis for comparison.\n\nAt the end of this process, it has been determined that there are now only 1.3 million articles left in totality. There are a suspicious amount of articles that are lost to being too new and I will be looking at that later, but for now am just working on lemmatizing the text using different methods.\n\nHere is the breakdown of documents (that are currently stored in separate files according to the number of processes):\n\n```bash\n[acarrab@login001 lemmatizedText]$ wc -l Abstracts_* | grep total\n   1328035 total\n```\n\nHere is the breakdown of how the documents were lost.\n\n```bash\n[acarrab@login001 acarrab]$ wc -l *.log\n  151427 failedDuringParsingOrNoAbstract.log\n   53872 hasAbstractAndFailed.log\n  164944 tooNewButHadAbstract.log\n  370243 total\n```\n\nNOTE: `tooNewButHadAbstract.log` is not necessarily accurate since it would put 3000 as year if the parsing could not find the date. \n\nI have looked at lemmatization through the means of using specialist NLP tool in order to use those to lemmatize the text, but to no avail. (A lot more learning than initially thought was needed is needed in order to use their tools).\n\nIn order to get some results, used the NLTK\'s lemmatizer in order lemmatize the text from the research papers. The papers are now ready for the next phase, the building of the n-grams.\n\nAfter speaking with Justin, he has seen that the benefits of using the Specialist NLP Tools is actually too slow for our data-set size in the end anyways and NLTK lemmatization does a good job and gives promising and significant results within the research that he has done.\n\n### Increasing valid parsing rate\n\n*Monday, October 16th*\n\nAfter running the jobs in order to parse the documents, added basic statistics capturing that are added to files while running batch job on the almost 1.8 million documents. At the end of the run, there are counts for different types of errors\n\n- Failure to parse xml errors\n\n- Had abstract keyword within file, but still failed\n\n- Failed for other reasons\n\nGiven this information, I can now look at the files that failed yet had the abstract keyword and determine how to reduce the number of files that are failed to be parsed.\n\n### Quality analysis of xml parsing\n\n*Wednesday, October 11th*\n\nWent through a random subset of the documents and looked at results from parsing in order to determine whether the parsing was doing what it should be doing. After making some modifications to the code written in the previous week, there is a good likelihood of valid parsing.\n\nAlso inserted failure cases for parse if things like the abstracts were missing.\n\nRan parsing parallelized on palmetto cluster and looked determined results so far. More validation should be done in order to make sure that most of the data are being used.\n\nFiles are saved in only abstract format and also full-text and abstract format in order to create distinct sets of documents that can give good comparison.\n\n### Heirarchical parsing of xml documents\n\n*Wednesday, October 4th*\n\nWrote code to parse out unicode characters as well as select out abstracts and other relevant text sections from the millions of documents.\n\nCreated method for parsing out xml documents tags that are needed in with hierarchical selection.\n\n#### Example and Explanation\n\nWrote code that extracts the following information by populating a data object with the keywords under the `\"GET\"` keyword under selection through chains of nested objects. For example, if an article title is found in the xml tree under `<article-meta><title-group><article-title>` tags, then `data[\"Title\"]` will be set to an array containing the found data.\n\nThis is a kind of cool way to extract the required information from the xml tree structure. Kind of inspired by [graphql](http://graphql.org/) selection statements in order to populate arrays of data.\n\n```python\nchains = [\n    {\n        \"article-meta\": {\n            \"title-group\": {\n                \"article-title\": {\n                    \"GET\": \"Title\"\n                }\n            }\n        },\n        \"kwd-group\": {\n            \"kwd\": {\n                \"GET\": \"Keywords\"\n            }\n        },\n        \"contrib-group\": {\n            \"contrib\": {\n                \"GETALL\": \"Contributors\"\n            }\n        },\n```\n\n### Downloading Documents\n\n*Thursday, September 28th*\n\nDownloaded the documents over ftp from [PubMed Central](https://www.ncbi.nlm.nih.gov/pmc/) and moved them onto the Palmetto Cluster.\n\nBecame familiar with some of [NLTK](http://www.nltk.org/) and processes like lemmatization and stemming. We will be using lemmatization for our work.\n\n### Data Selection Decision\n\n*Wednesday, September 20th*\n\nAfter looking through different sources of data this week and talking with my research mentor, it has been decided that using [PubMed Central](https://www.ncbi.nlm.nih.gov/pmc/) as the sole data source should give enough documents to have good results. Furthermore, the data source contains parsed documents in XML format which allows for minimal use of additional parsing techniques from PDF format.\n\n### Read through papers\n\n*Wednesday, September 13th*\n\nAfter being introduced to subjects by reading research papers, I have a decent understanding of the process that is taken to get from the step of text extraction and input to hypothesis generation (with lack of understanding of some specifics). I will be speaking with Justin some time this week in order to work out some questions I have about the process, but overall seems like a very cool process.\n\nI am now starting to look through different sources of data. The main and quickest source to find is [PubMed Central](https://www.ncbi.nlm.nih.gov/pmc/), but—given that this should have on the order of a couple of million of articles—this could be enough if no other viable source of data is found.\n\nI have also been looking into different programs for parsing text from PDF format.\n### Beginning of Research\n\n*Wednesday, September 6th*\n\nMet with Dr. Safro and Dr. Herzog, and Justin Sybrandt from the ACS Lab. Introduced to and talked with Justin about his research and where my additional work will fit in. \n\nWhat I will be doing is working on reapplying Topic Modeling and Hypothesis Generation with abstracts and also full-text research papers as data input. This will require:\n\n1. Reading through paper on [MOLIERE: Automatic Biomedical Hypothesis Generation System](https://arxiv.org/abs/1702.06176) as well as others in order to become introduced to research at large within this particular area.\n    - Others Include\n        - [The structural and content aspects of abstracts versus bodies of full text journal articles are different](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-11-492)\n        - [Text mining of 15 million full-text scientific articles](https://www.biorxiv.org/content/biorxiv/early/2017/07/11/162099.full.pdf)\n\n1. Finding of papers in large enough size so that reliable results and valid comparisons can be made. \n1. Parsing out text from papers and removing things like, equations, tables, image links and references."} />)} />
            <Route path='/projects/multi-robot_environment' component={() => (<ReactMarkdown source={"# Research Science Internship @ DIMACS REU at Rutgers University\n\nFeel free to download and look at my [research paper](../Resources/finalResearchPaper.pdf).\n\n\n## Discrete Representation of multi-agent environment\n\n![](../Resources/DiscreteSmaller.gif)\n<center><em>Discrete Multi-Robot Environment Simulation</em></center>\n\n### Diagram explanation\n\nThis is a simulation of a multi-robot environment in which basic path\nplanning is implemented. It is done in a discrete case to allow for easier\npath planning. The grid size and hexagonal shape is chosen so that a disc\nwith radius **r** will not collide with another disc of radius **r**.\n\nPseudo code\n\n1. A Starting and Ending point on the graph are randomly chosen for each disc.\n1. A breadth first search is done from each discs goal node, that is saved on the node data structure\n1. On each discrete transition, every node tries to move closer to its goal node, while trying not to run into another disc, using info on the nodes\n1. Random priorities are decided among the discs and the next step is computed.\n1. Repeat step 4 until all nodes are at their respective goal states.\n\n## Work Summary And Blog\n\nFor this project, I was responsible for generating simulations for\nmulti-robot environments. These simulations would be used in order to\ndetermine if there are any sharp-transitions in the behavior of\nmulti-robot systems when the constraints of the system changed,\nspecifically the locality and density of robots in the system.\n\nOverall, I looked at three different scenarios under these constraints:\nContinuous, Discrete, and Discrete with Collision avoidance. Often times\nproblems that have to do with a continuous amount of possibilities is\nsimplified to a discrete case to stop it from being able to be solved in\na reasonable amount of time, so that was why continuous with path\nplanning was not done.\n\nIn the end, it was determined that there were no sharp transitions in\nthe specific environments and scenarios that I explored. Further\nresearch can be done to see what types of properties would lead to such\ntransitions, but as of now my simulations and path determination\nalgorithm do not create sharp transitions.\n\n![](../Resources/ContinuousSmaller.gif)\n\n<center><em>Continuous Case</em></center>\n\n## Final week and a Research Paper\n\n*Week 9*\n\nThis week, I am continuing to write my final research paper based upon\nthe results that were found. I would like to thank DIMACS and NSF for\nthe funding that I received and also for the great Summer of Research\nthat I had; it was an unforgettable experience.\n\n## Final data collection\n\n*Week 8*\n\nThis week, I am collecting more data. Also, working on writing the final\nresearch paper and the summary of my experience in the program.\n\n\n## Lack of a sharp transition\n\n*Week 7*\n\nThis week, I worked more with locality and discovered moreso how it\naffects the number of collisions overall. There were no unexpected\nresults however and there was also no perceiveable sharp transition. So\nfar it seems like I would have to increase the sophistication for the\ncollision avoidance to get a sharp transition.\n\n##  Locality and disc radius\n\n*Week 6*\n\nThis week, I worked more on data collection and exploring the solution.\nI implemented the locality based on edge distance generation for\nstarting and ending nodes for the discs. I also collected data based on\nthe locality and total discs in the environment as without the locality\nthe results were predictable and there was no sharp transition.\n\n## More Data and more deterministic\n\n*Week 5*\n\nI gathered basic data on the collision behavior, but the more random\nmethod of determining solutions to the problem caused a few problems\nwith the speed of solving and thus the speed of collecting data. I\nworked this week on making the process deterministic. I also collected\nsome more data.\n\n## Trying out the GPU\n\n*Week 4*\n\nThis week, I generated graphs based on how the locality of travel and\ndensity of the discs area in a unit square affect the number of\ncollisions. Using plot.ly along with python, I was able to run\nsimulations on the number of pairwise collisions. In the system there\nappeared to be no sharp transitions.\n\n\n## I now have graphs\n\n*Week 3*\n\n This week, I generated graphs based on how the locality of travel and\ndensity of the discs area in a unit square affect the number of\ncollisions. Using plot.ly along with python, I was able to run\nsimulations on the number of pairwise collisions. In the system there\nappeared to be no sharp transitions.\n\n## Onwards!\n\n*Week 2*\n\n This week, I ran simulations to determine how the properties of the\nsystem affected the interaction between the discs. The results of this\nstep are so far inconclusive since no sharp transition was found yet. I\nset up python and pygame within visual studios and then started to work\nin/learn python to create a grid for the discs to be on. This will be\nuseful later for creating models with pygame.\n\n\n## Start of the Program\n\n*Week 1*\n\n This week I worked on putting together a presentation as well as making\nsome headway into gathering data on how the number of discs, the radius,\nand maximum distance from starting point to ending point affect the\nnumber of pairwise collisions that occur. I layed out a plan for what I\nam going to be researching as well as getting my workspace organized and\nset up for working. I met with my mentor, Jingjin Yu, and we layed out\nmore-so what I will be starting with."} />)} />
            <Route path='/projects/pubsub_architecture_analysis' component={() => (<ReactMarkdown source={"# Research with Human-Centered Cloud Robotics group @ Clemson\n## Simulation of network\'s effect on real-time environemnt\n![](../Resources/pubSubRep.png)\n### Tools for Analysis\n\n*Tuesday, November 15th, 2016*\n\nI mentioned previously that I will be looking at how well different publish and subscribe architectures perform in real time environments.\nThe specific environment that I am looking at is one in which a controller will be controlling, through a broker and a plant, a tiltable plate with a ball on it.\nthis allows for us to analyze the behavior over time and how well it actually performs the task, without any predictive behaviors.\nThe following figures are the basis of my visual analysis, since the ball-plate model is a simulation in the first place. There are really just\ntwo things that I am looking at right now, how far the ball is from its desired location in the x and y direction and a histogram of how long the sending of\ndata actually takes.\n\n![](../Resources/figure_1.png)\n\n<center><em>Distribution of Time Taken</em></center>\n\n![](../Resources/figure_2.png)\n![](../Resources/figure_3.png)\n\n<center><em>How Far from Path</em></center>\n\nThese diagrams will generally help with basic analysis, but numerical analysis and comparisons will\nbe done as well.\n\n### Mininet\n\n*Wednesday, November 2nd, 2016*\n\nOver the past few weeks, I have been working on creating network topologies with Mininet[1], then\ntesting the performance of MQTT QoS 2 in order to run a ball-plate simulation. A ball-plate\nsimulation is one in which we literally simulate three entities: a ball being balanced on plate, an\nobserver (plant), and a controller. The observer desires the ball to be in a certain location on the\nplate and also in constant motion. The controller just listens the position that the observer wants and\ntries to make it happen if it can.\n\nNow the specific network topology only consists of 3 hosts and their respective switches; the hosts are\ncalled Broker, Plant, and Controller. The topology is shown in the diagram below.\n\n![](../Resources/diagram1.png)\n\n<center><em>Network Topology</em></center>\n\n\nAlthough a basic topology, it will be perfect for simple analysis of the effect of latency and\ntime delay between transmissions for the data between the host, broker, and plant. Originally there were\nsome problems with creating the network topology, because I was not running a controller on my machine.\n\n### Simulating Network Topology with Mininet\n\n*Wednesday, October 12th, 2016*\n\n I have also been looking into some of the features of Mininet, which is\nan application (or an extension of Linux) that allows you to construct a\nnetwork and create connections between virtual hosts and servers within\na network. Actually, the specific structure that I am wanting to create\nis one in which there is a publisher, a subscriber, and a broker or a\ncontroller, plant, and broker. After I achieve that, I will be adding more\nand more subscribers and/or publishers and then study the amount of traffic\nthat goes through the network and how real time tasks are affected by the\nchanges on the load within the server and Network. I will then be adding\nKafka and MQTT into the mix and comparing the two. Not only comparing\ngeneric MQTT and Kafka, but the three levels of MQTT (explained below)\nand Kafka. Right now, I am currently working more and more with Mininet\nin order to learn how to accomplish these tasks. [3]\n\n### Learning more about Kafka and MQTT\n\n*Tuesday, October 11th, 2016*\n\n Over the past week I have been looking more into Kafka and MQTT in order to\nget a better understanding of their purposes and drawbacks. I spoke with my\nmentor, Dr. Remy, and looked at some of the drawbacks and benefits of these\npublish and subscribe architecture with respect to the amount of overhead\nthat is associated with each. Overall, it seems that Kafka processes less\ninformation in the server and then sends more data in order to achieve the\nsame task on real-time process, whereas MQTT seems to process more within\nthe servers and then send less data overall. Hence the architecture, MQTT,\nliving up to its name of mosquito. MQTT has three levels of service, unlike\nKafka which is always guaranteed at least once delivery; MQTT can be at most\nonce but maybe not at all (QoS 0), at least once (QoS 1), and at most once\n(QoS 2) whereas Kafka is usually only guaranteeing at least once delivery.\nThis creates the possibility for a lot of data being sent over the network,\nbut is also quicker than at most once delivery, because of the lack of a 3\nway handshake. [1, 2] I am curious about what the size of the Kafka messages\nwere that were stored on the server, because if the link to the next message\nis just as long as the message, then we will definitely get a much larger\namount of data sent. Overall, it seems that the larger the message being sent\nin Kafka, the more the overhead shrinks in comparison; in other words, the\nheader does not need to change size when the message size changes by that\nmuch so the theoretically optimal thing to do is to have a message of\ninfinite length and then the overhead would be zero. (not really feasible but\nmakes mathematical sense).\n\n### Learning More about TCP\n\n*Monday, October 10th, 2016*\n\n Last week I worked on our Networking class project for TCP. I started out\nwith just a basic skeleton file with the basic necessities of establishing\na TCP connection between an echo server and an echo client. I made a basic\noverarching protocol where header information was sent first with the\namount of bytes that were going to be sent, and the number of messages that\nwould be sent. With this, I could declare if there were going to be more\nmessages and how many more would be sent. This way there would be no way\nfor the server to close the connection before the client was done\ncommunicating with the server. The server then did the simple operation of\njust inverting the characters in the array; it just switched the case of\nthe letters. This was a toy project just to learn how to use TCP and how\nto guarantee that the connection does not close before the message is done\nsending.\n\n### Dweepy and Dweet.io Analysis\n\n*Tuesday, September 6th, 2016*\n\n This past week I worked on creating software for in depth analysis of the\nsending and receiving procedure for dweet.io! I started with creating\nsoftware to time how long it takes for a message to be published and a\nmessage to be received when using dweepy. Then, using tcpdump, I analyzed\nhow many messages were sent and received. Using pexpect, a python library\nfor scripting, I ran tcpdump as a child process and used it to analyze\nthings like, how many sends, time between sends and receives, the number\nof sends and receives, the number of bytes sent, and finally the average\nordering of the send receive procedures as well as the percentage of n\nmessages being sent over the trials.\n\n After collecting the data, we can analyze the amount of overhead that\nexists for sending an integer (4 bytes) and a string of length 10\n(10 bytes) for a total of 14 bytes. For both publishing and subscribing,\nwe receive 5,500 bytes and send 1,500 bytes. Also, I looked at the time it\ntakes to get from the dweet.io to the first requests, which doesn\'t seem to\nbe that long on average. Most of the time was spent during the send and\nreceives between the server and client; there were 16 send/receives in\ntotality.\n\n One problem with my method was that I was waiting 1 second with pexpect\nin order to see if anymore messages would be sent and received, but this\ngave the possibility for outliers to be thrown into the mix which\nresulted in some strange time problems. (Negative time results) Overall,\nthe quality of the results so far is best when looking at the average\nordering and the number of bytes sent. In order to improve, I will rerun\nthe trials with a longer delay in between functions in order to catch the\noutliers that take a long time to send a response.\n\n It is interesting that so much data is actually sent to publish such a\nsmall message, but further research must be done to see what can be\nremoved from the process. I will start working with pycurl and implement\nmy own method for communicating with the server and see how that affects\nthe overall times."} />)} />
            <Route path='/projects/building_websites' component={() => (<ReactMarkdown source={"# My Website\n\n**Built with react, autogenerated from markdown documents and directory structure, with bootstrap layered on top of it**\n\nThis website is compiled and then statically served, which makes it good for use on any web server.\nI know now that there is [Hugo](https://gohugo.io/), but I enjoyed the process of putting all this together.\n\nAll the html that is presented before you, is generated directly from the directory layout of files. Directories are turned into drop-down menus. (This does however become difficult when it is a multi-level drop-down, which I plan to have if time allows)\n\nCode is on [Github](https://github.com/acarrab/acarrab.github.io)\n\n## There was also my other website\n\nThis [older website](/OldWebsite) was built for fun and from a low level. I think it is really fun to travel around though! I made ***interesting*** design choices though...\n\nI would have kept using this, if it wasn\'t for the need to have good mobile support.\n\n[![](../Resources/OldWebsiteScreenCap.png)](/OldWebsite)"} />)} />
            <Route path='/projects/deep_learning' component={() => (<ReactMarkdown source={"# Deep learning Projects\n\nRight now, I am working on a handful of projects that pertain to deep learning as well as making my way through\n[this book](http://www.deeplearningbook.org/). This started from long-running interest in machine learning.\nAfter taking some of the math classes that pertain to deep learning (Linear Algebra, Discrete Mathematics, Theory of Probability, Differential Equations, and Multi-variable Calculus), I want to learn deep learning in *depth*.\n\n## Senior Capstone Project\n\nCurrently working on a senior capstone project in which we have been entrusted 2 **NVIDIA Tesla P100 GPUs**. These are GPUs that are specifically made to tackle some of the biggest problems in Machine Learning. Our group has been working on creating the proper infrastructure that will be able to efficiently and without additional bottlenecks.\n\nI specifically have been researching different hardware requirements for the GPUs, working on finding some good examples that showcase their powerful performance. So far, our group--through one of our clemson faculty members--is ordering components, like a server rack and other necessities.\n\n### Goals for the project\n\nWe are working on a handful of goals, but here are the basics\n\n1. Setting up Tesla P100 Architecture\n1. Take online classes in order to learn more about deep-learning frameworks\n1. Implement some projects within the system after setup\n1. Benchmark the performance of the machine against other machines in the department\n1. Create tutorial tailored for students in order to give them familiarity with the system\n1. Create fun example in order to showcase one of many cool projects within the Computer Science Department at Clemson\n\n## Technical Writing\n\n(Yes even within Technical Writing)\n\nOur class is making a showcase and giving a presentation of the new technologies of that are available through new IoT/Big Data.\nBeyond that, a large aspect of the project is our groups teaming up with groups at University of Braunschweig in Germany.\n\nOur group, on the other hand, is taking this opportunity to showcase real-time Object-Recognition and Classification using some neat libraries. (However, we have not yet decided between Tensor Flow, DIGITS, or Watson\'s object recognition software)\n\n## Personal Goals\n\nI am working on learning more of the math in order to be able to create tailored machine learning models in order to tackle real-world problems. However, I do not want to use something so cool and not really know what is going on underneath the hood.\n\nMy routines for learning consist of\n\n- Reading through [this textbook](http://www.deeplearningbook.org/)\n- Watching [Siraj](https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A)\n- Trying to get into a course on [Udacity](https://www.udacity.com/) for deep learning\n"} />)} />
            <Route path='/school_work/2015_Spring_102_RayTracer' component={() => (<ReactMarkdown source={"# Ray Tracer\n\n*Spring 2015*\n\nWrote ray-tracer code in order to render a spaceship. The core aspects of the class were to be introduced to c++, but using additional knowledge gained through classes like Multi-Variable calculus, I was able to create unique 3d structures. Also implemented texture mapping to a sphere, which is showcased in the photo.\n\n### Result\n\n![](../Resources/SpaceRaytracer.jpg)"} />)} />
            <Route path='/school_work/2017_Spring_Computer_Graphics' component={() => (<ReactMarkdown source={"# Computer Graphics Projects\n\n## Teapot with Rendering Equation approximation\n\n*Late Spring 2016*\n\nThis project was done in order to approximate the light scattering effects that occur within real-world environments. \n\nUsing [Radiosity equations](http://graphics.stanford.edu/courses/cs348b-10/lectures/renderingequation/renderingequation.pdf) and randomized sampling around spheres according approximate things like light scatter as well.\n\n### Result\n\n![](../Resources/renderingEquationTeapot.png)\n\n\n## Stanford Bunny\n\n*Early Spring 2016*\n\nFor this project, our group worked with the classic stanford\nbunny and applied some of our newly learned knowledge in computer\ngraphics to generate a model of the bunny and make it look pretty.\n\nIn hindsight, the motion blur was too much :(\n\n![](../Resources/stanfordBunny.png)"} />)} />
            <Route path='/school_work/2017_Spring_2D_Game_Engine_Design' component={() => (<ReactMarkdown source={"# 2D Game Engine Design\n\n*Spring 2017*\n\nThis was a class intending on improving object oriented code quality and c++ understanding. All students worked on a semester long project with the end result of having a game being built and functional. For this task, the only tool we used other than stl was the [SDL (Simple Direct Media Layer) 2.0](https://www.libsdl.org/) framework.\n\n## My game\n\nI wanted to do something where I incorporated fun and amusing particle physics within the game. What better way to do this than to do this with a particle wizard! Using quick collision checking methods and some design patterns to improve things like speed and memory efficiency, the game is able to support the firing and handling of many particles in the system.\n\n<video width=\'640\' height=\'480\' controls>\n<source src=\'../Resources/WizardGame.mp4\' type=\'video/mp4\'>\n</source>\n</video>\n"} />)} />
            <Route path='/school_work/fun_courses' component={() => (<ReactMarkdown source={"# Fun/Recommended Clemson Courses to take\n\nThese are classes I have taken out of interest or thought would help me with my academic goals.\n\n### Fall 2017\n\n- Senior Capstone Project 4910\n  - Focusing on learning deep learning concepts and some infrastructure specifics\n- Programming Systems\n  - The *how it\'s made* of programming languages\n\n### Spring 2017\n\n- Design and Analysis of Algorithms 8400\n  - Learned advanced runtime analysis and better application of known algorithms\n- Data Science\n  - Learned high level information about statistical models\n- Computer Graphics\n  - Fun geometry and calculus as well as well as the need to work in highly constrained environment\n\n### Fall 2016\n\n- Networks and Network Programming\n  - Worked on Honors research (with Human-Centered Cloud Robotics) while in this class\n\n### Spring 2016\n\n- Ordinary Differential Equations (Honors Version)\n- Linear Algebra\n\n### Fall 2015\n\n- Algorithms and Data Structures\n  - Still in top 5 favorite classes\n- Organic Chemistry\n  - Fun class that teaches students to work with abstract, feature-described, objects and try to predict their interactions.\n\n### Spring 2015\n\n- Calculus of Several Variables\n- Comp Sci 102\n  - First project heavy computer science course\n  - This is where I made [the ray-tracer](#/school_work/2015_Spring_|_102_RayTracer) and code for modeling the scene"} />)} />
        </Switch>
    );
}